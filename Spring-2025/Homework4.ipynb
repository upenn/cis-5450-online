{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2L0PkeSIuOD"
   },
   "source": [
    "# CIS 5450 Homework 4: Machine Learning\n",
    "\n",
    "<ins>Due Date</ins>: **March 24th at 11:59PM EST** \\\\\n",
    "<ins>Total Points</ins>: **95 points** (= 84 autograded + 11 manually graded)\n",
    "\n",
    "**PLEASE READ THE FAQ as you do this assignment!** It's pinned on Ed and we TAs work really hard to keep it updated with everything you might need to know or anything we might have failed to specify. Writing these HWs and test cases gets tricky since students always end up implementing solutions that we did not anticipate and thus could not have prepared the grader correctly for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETZ9CDIDLYHp"
   },
   "source": [
    "# Imports/Setup\n",
    "Run the following cells to set up the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEUIzvEuMdGQ"
   },
   "source": [
    "Please make sure you enter your **8 digit Penn ID** in the  student ID field below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adECTPlZLtVi"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -i https://test.pypi.org/simple/ penn-grader==0.5.0\n",
    "from penngrader.grader import *\n",
    "\n",
    "# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY.\n",
    "# IF NOT, THE AUTOGRADER WON'T KNOW WHO TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
    "STUDENT_ID =                                                                    # YOUR PENN-ID GOES HERE AS AN INTEGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAWRpROskV1E"
   },
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
    "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IiUMGpQukYHj"
   },
   "outputs": [],
   "source": [
    "grader = PennGrader('config.yaml', 'cis5450o_spr25_HW4', STUDENT_ID, STUDENT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBTaESEdMmiU"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "from google.colab import drive\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tA6oTIZtRjf7"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt update\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odlXF5IYJ-et"
   },
   "source": [
    "# **Part I:** Preprocessing and Modeling in `scikit-learn` (65 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XSw-F5VjVKI"
   },
   "source": [
    "## **1.1** Data Loading and Preprocessing [Total: 2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Sw7LCAacKOu"
   },
   "source": [
    "### **1.1.1** Read Data\n",
    "\n",
    "We are using one CSV for this homework, `Hotel Reservations.csv` from a Kaggle [dataset](https://www.kaggle.com/datasets/ahsan81/hotel-reservations-classification-dataset?resource=download). The dataset contains a column called `booking_status`, a binary variable indicating whether a reservation was canceled or not.\n",
    "\n",
    "To get the data in here:\n",
    "1. Go to this [Kaggle link](https://www.kaggle.com) and create a Kaggle account (unless you already have one)\n",
    "2. Go to Account and click on \"Create New API Token\" to get the API key in the form of a json file `kaggle.json`\n",
    "3. Upload the `kaggle.json` file to the default location in your Google Drive, 'MyDrive' (Please **DO NOT** upload the json file into any _specific_ folder as it will be difficult for us to debug issues if you deviate from these instructions!).\n",
    "\n",
    "This can be helpful for your project if you decide to use Kaggle.\n",
    "Run the following cells to allow Colab to connect to Kaggle and to download the data used in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGz24WQ86ptM"
   },
   "outputs": [],
   "source": [
    "# Run this cell to mount your drive (you will be prompted to sign in)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4taOIpPb6CN"
   },
   "outputs": [],
   "source": [
    "# Create the kaggle directory and\n",
    "# (NOTE: Do NOT run this cell more than once unless restarting kernel)\n",
    "!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uO1UNkV6cbxA"
   },
   "outputs": [],
   "source": [
    "# Read the uploaded kaggle.json file\n",
    "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krFCjQBzcdth"
   },
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "!!kaggle datasets download -d ahsan81/hotel-reservations-classification-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1e75jxBctPG"
   },
   "outputs": [],
   "source": [
    "# Unzip folder in Colab content folder\n",
    "!unzip /content/hotel-reservations-classification-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PB4RPNywcKOv"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Read the csv file and save it to a dataframe called \"df_reservations\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0FYkm8pkQGu"
   },
   "outputs": [],
   "source": [
    "# CHECK: Peak at the first five rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJ9m-tIgiYRe"
   },
   "source": [
    "### **1.1.2** Check Nulls and Duplicates\n",
    "\n",
    "We will find the number of rows with null values and the number of duplicated rows.\n",
    "\n",
    "Store the results into `num_nulls` and `num_dups`, respectively.\n",
    "\n",
    "**Please use pandas functions to compute these values instead of hardcoding them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkLzEgHWOcEU"
   },
   "outputs": [],
   "source": [
    "# TO-DO: find number of rows with null values\n",
    "num_nulls ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLehMDouqJsH"
   },
   "outputs": [],
   "source": [
    "# TO-DO: find number of duplicated rows\n",
    "num_dups ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYhoktAQi1eq"
   },
   "outputs": [],
   "source": [
    "#Grader Cell (2 points)\n",
    "grader.grade(test_case_id = 'drop_null_and_dups', answer = (num_nulls, num_dups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsDRMCdfkACq"
   },
   "source": [
    "## **1.2** EDA [Total: 16 points]\n",
    "\n",
    "Exploratory Data Analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngsCDv0RkCKf"
   },
   "source": [
    "### **1.2.1** Understanding Data\n",
    "\n",
    "A good practice before approaching any data science problem, is to understand the data you will be working with. This can be through descriptive statistics, datatypes, or just a quick tabular visualization. We will be walking through such tasks through Pandas.\n",
    "\n",
    "While not explicitly graded, if you don't do this, you are more likely to make mistakes down the line if you don't have a good understanding of your datasets and their range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P60Qyz-okvxk"
   },
   "outputs": [],
   "source": [
    "# Display the datatypes in `df_reservations`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kIN2As8lIkn"
   },
   "outputs": [],
   "source": [
    "# Display the descriptive statistics of `df_reservations`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehKm4W7_wlu1"
   },
   "source": [
    "We can do further visualization in the form of plots and graphs if we want to explore the data further, but in the case of this homework we stop now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_uvTv_VKYfY"
   },
   "source": [
    "### **1.2.2** Correlation of Feature Variables\n",
    "\n",
    "With multiple features, it can be somewhat exhausting to do bivariate analysis on every possible pair of features. While you certainly should, your first instinct should be to check for the correlation between features since certain models (e.g. Linear Regression) will not work well if we have strong multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YacG1jN-_WI6"
   },
   "source": [
    "_**Isolating Numerics from Categorical Features**_\n",
    "\n",
    "Before anything else, it may help to create groups of the numeric and categorical variables.\n",
    "\n",
    "**Task:** is to split the `df_reservations` dataframe into 2 dataframes:\n",
    "\n",
    "1. `numerics_df`: This dataframe contains all numerical columns from `df_reservations`\n",
    "\n",
    "2. `categorical_df`: This dataframe contains all categorical columns from `df_reservations`\n",
    "  - i.e. the columns of non-numeric type or contain binary values\n",
    "\n",
    "In the above dataframes, **please do not include `arrival_year`, `arrival_month`, `arrival_date`, `Booking_ID`, `booking_status`**.\n",
    "This is because:\n",
    "\n",
    "* For the sake of this exercise we will assume that time related features ( `arrival_year`, `arrival_month`, `arrival_date`) can not be categorized as numeric or categorical\n",
    "* We will use `booking_status` as the target variable for classification\n",
    "* `Booking_ID` is just an identifier and does not hold any predictive information.\n",
    "\n",
    "Also, **sort the columns of `numerics_df` and `categorical_df` in alphabetic order**\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "Please use 'numerics_df' and 'categorical_df' as your dataframe names so that the autograder can recognize them.\n",
    "\n",
    "Internal Server Error occurs when your output crashes the test case in the autograder.\n",
    "\n",
    "**Hint:** Do not just automatically assume that if something is an integer it should be part of numerics_df.\n",
    "For example if something has a yes/no answer (binary random variable) such as a column indicating whether someone is female or not female, this is technically categorical data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBxOkIQp_B_w"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Visualize number of unique values and datatype in each column (call .nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZIPC3qoLgag"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Form 'numerics_df' and 'categorical_df'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kl0qUiGjOYnZ"
   },
   "outputs": [],
   "source": [
    "# TO-DO: sort the columns of `numerics_df` and `categorical_df` in alphabetic order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGWnwyl4Bkjt"
   },
   "outputs": [],
   "source": [
    "#Grader Cell (3 points)\n",
    "grader.grade(test_case_id = 'cat_num_df', answer = (numerics_df, categorical_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRaftmxV-FEV"
   },
   "source": [
    "#### (b) **Correlation Heatmap** [3 pts]\n",
    "\n",
    "**Manually Graded Task:** Create a correlation matrix using `numerics_df` and call it `corr_mat`. Using the correlation matrix, generate a correlation heatmap for these numeric features. We will be using Seaborn library to create this heatmap.\n",
    "\n",
    "Make sure your correlation heatmap meets the following criteria:\n",
    "*   Ensure that your heatmap is of figure size `(8,8)`: all feature labels should be visible on both the $x$-axis and $y$-axis\n",
    "*   Use the `RdBu` color map to ensure that negative correlations are red and positive correlations are blue. This is far more intuitive than other ones.\n",
    "*   Standardize the color scale so that -1 takes the darkest red color, 0 is totally white, and +1 takes the darkest blue color. Remember, the Official Documentation should always be your first point of reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IjmGO3eLcjz"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Plot correlation heatmap (3 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06OFh_eTNIPI"
   },
   "source": [
    "## **1.3** Feature Engineering [Total: 4 points]\n",
    "\n",
    "Feature engineering is the process of applying domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJ2ptxmkNyG6"
   },
   "source": [
    "### **1.3.1** One Hot Encoding [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIaH3ZHsNvei"
   },
   "source": [
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. With one-hot encoding, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns.\n",
    "\n",
    "One way to one-hot encode in Pandas is by using `pd.get_dummies()` . The documentation can be found [here](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html).\n",
    "\n",
    "**Input:** Perform operations on `df_reservations`\n",
    "\n",
    "**Task:**\n",
    "- Use `pd.get_dummies()` to one hot encode the following columns:\n",
    "  - `\"type_of_meal_plan\"`\n",
    "  - `\"room_type_reserved\"`\n",
    "  - `\"market_segment_type\"`\n",
    "- Save your result in the dataframe `encoded_df_reservations`.\n",
    "\n",
    "**Note:** Do <ins>NOT</ins> modify the column names. Use the default names `pd.get_dummies()` generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttMWoxUPOhqF"
   },
   "outputs": [],
   "source": [
    "# TO-DO: drop \"Booking_ID\" column from `df_reservations`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCwdnXBRCNe-"
   },
   "outputs": [],
   "source": [
    "# TO-DO: create dataframe 'encoded_df_reservations' that contains the appropriate one hot encoded columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvEcv14XGrV2"
   },
   "outputs": [],
   "source": [
    "# CHECK: display the first two rows of 'encoded_df_reservations'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sa082oxXE8F-"
   },
   "outputs": [],
   "source": [
    "#Grader Cell (2 points)\n",
    "grader.grade(test_case_id = 'one_hot_encoded', answer = encoded_df_reservations.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bVkkvxRrWuc"
   },
   "source": [
    "### **1.3.2** Change 'booking_status' to Boolean [2 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcbPXfEuEy6s"
   },
   "outputs": [],
   "source": [
    "# TO-DO: canceled = 1, not canceled = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILEHbWkbrrfi"
   },
   "outputs": [],
   "source": [
    "# Grader Cell (2 points)\n",
    "grader.grade(test_case_id = 'boolean_booking', answer = encoded_df_reservations.head(20)['booking_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-iqlFZacKO9"
   },
   "source": [
    "## **1.4** Modeling (sklearn) [Total: 43 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPBSUmK9HRHc"
   },
   "source": [
    "### **1.4.1** Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtCmlzfGwlu4"
   },
   "source": [
    "#### (a) Create Features and Label [4 pts]\n",
    "\n",
    "Now that we have explored and cleaned our dataset, let's prepare it for a machine learning task. In this homework, you will work with various models and attempt to predict whether a room booking will be cancelled or not.\n",
    "\n",
    "The features will be all the variables in the dataset **except** `\"booking_status\"`, which will act as the label for our problem. First, store these two as `features` (pd.DataFrame) and `target` (pd.Series), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7_jhB-LEVNK"
   },
   "outputs": [],
   "source": [
    "# TO-DO: stores features dataframe into variable called \"features\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAuxd7Q2rbYO"
   },
   "outputs": [],
   "source": [
    "# TO-DO: store the classification target variable into \"target\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4IK9S59ud0Q"
   },
   "source": [
    "Now, use scikit-learn's `train_test_split` function to split data for regression into train and test sets. The split should be 80-20 meaning 80% for training and rest for testing.\n",
    "\n",
    "**_IMPORTANT_:** Please set the `seed` variable to 42, then set the parameter to `random_state = seed`, and then finally store the resulting splits as `X_train, X_test, y_train,` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D48UmEtDEo0g"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Assign appropriate value to seed and conduct 80/20 train-test split with random_state = seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4SMMl8JEqnT"
   },
   "outputs": [],
   "source": [
    "# Grader Cell (4 points)\n",
    "grader.grade(test_case_id = 'train_test_split_classification', answer = (X_train.shape, X_test.shape,\n",
    "                                                                         y_train.shape, y_test.shape,\n",
    "                                                                         seed, Counter(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQid9KfyIN2-"
   },
   "source": [
    "### **1.4.2** Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1Vj9AkIIgpL"
   },
   "source": [
    "#### (a) Logistic Regression [2 pts]\n",
    "\n",
    "Fit a Logistic regression classifier on the `X_train` and `y_train` with the default hyperparameters. Calculate the accuracy of the model on the test set using the `score` method and store it in a variable named `log_acc`\n",
    "\n",
    "If you receive warnings when training your model, you can ignore them for this homework.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "Don't worry about convergence warnings for this HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cS9UpoTmEsin"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Import required libraries\n",
    "\n",
    "\n",
    "# TO-DO: Initialize model with default parameters and fit it on the training set\n",
    "\n",
    "\n",
    "# TO-DO: Use the model to predict on the test set and save these predictions as `y_pred`\n",
    "\n",
    "\n",
    "# TO-DO: Find the accuracy and store the value in `log_acc`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyfS-7oIF88y"
   },
   "outputs": [],
   "source": [
    "# Grader Cell [2 points]\n",
    "grader.grade(test_case_id = 'check_log_clf', answer = log_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgbNIcEFMch4"
   },
   "source": [
    "#### (b) Random Forest Classifier [3 pts]\n",
    "\n",
    "Fit a Random Forest classifier on the `X_train` and `y_train` with the following hyperparameters:\n",
    "- balanced class_weight\n",
    "- 120 estimators\n",
    "- maximum depth of 30\n",
    "- random seed set to 42\n",
    "\n",
    "Calculate the accuracy of the model on the test set using the `score` method and store it in a variable named `rf_acc`.\n",
    "Also, compute a [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) for your prediction and save it as `rf_confusion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tk9YGVBVFTIe"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Import required libraries\n",
    "\n",
    "\n",
    "# TO-DO: Initialize model with default parameters and fit it on the training set\n",
    "\n",
    "\n",
    "# TO-DO: Use the model to predict on the test set and save these predictions as `y_pred`\n",
    "\n",
    "\n",
    "# TO-DO: Find the accuracy and store the value in `rf_acc`\n",
    "\n",
    "\n",
    "# TO-DO: Compute the confusion matrix and save it to `rf_confusion`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlpS3wDEE9-T"
   },
   "outputs": [],
   "source": [
    "# Grader Cell (3 points)\n",
    "grader.grade(test_case_id = 'check_rf_clf', answer = (rf_acc, rf_confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahdJZonewlu6"
   },
   "source": [
    "#### (c) Plot Feature Importances [5 Points]\n",
    "\n",
    "Now that you have a Random Forest model, it's important for data scientists to understand and communicate to stakeholders the drivers that cause booking cancellations. **Feature importance** can help achieve this.\n",
    "\n",
    "Here are the steps to follow:\n",
    "\n",
    "* Use the built-in feature_importances_ function to obtain an array of feature importances.\n",
    "* Identify the top 10 features by importance then create a DataFrame with two columns: one for the names of the current features (Called Feature) and another for their corresponding importance values (called Feature Importance).\n",
    "* Visualize the top 10 feature importance using a bar plot.\n",
    "* Analyze the top 10 features to determine if they make sense, and provide your reasoning (manual grading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xX8qCQqjwlu6"
   },
   "outputs": [],
   "source": [
    "# TO-DO: get feature importnce using built-in feature importance\n",
    "\n",
    "# TO-DO: Get the top 10 and Create a DataFrame `feature_importance_df`\n",
    "\n",
    "# TO-DO: Visualize the top 10 feature importance using a barplot\n",
    "\n",
    "# TO-DO: Analyze the top 10 features to determine if they make sense, and provide your reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MFaChAqwlu6"
   },
   "outputs": [],
   "source": [
    "#Grader Cell\n",
    "grader.grade(test_case_id = 'check_feature_importance', answer = (feature_importance_df.reset_index()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbSul1eKNIVG"
   },
   "source": [
    "#### (d) PCA to Reduce Dimensionality [4 + 2 pts]\n",
    "\n",
    "The goal of Principal Component Analysis is to reduce number of dimensions of our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caGTeWhxNL3K"
   },
   "source": [
    "_**Initial PCA**_\n",
    "\n",
    "As a first step, instantiate the `PCA` class from scikit-learn and fit it on your training set. We are not reducing the dimensionality of our data in this step but rather trying to find what would be the ideal number of Principal Components to choose. So keep all Principal Components for this step\n",
    "\n",
    "Please remember that PCA is **not** scale-invariant! What does this imply you need to do first? This requires conceptual understanding to implement so make sure you _watched_ the corresponding lecture and recitation!\n",
    "\n",
    "<ins>WARNING</ins>: It is _CRITICALLY_ important that you understand the difference between the `.fit_transform`, `.fit` and `.transform` methods. This has been the most common mistake students have made in this homework, which has potentially made it more difficult for them. Without fully understanding this, those same students have gone on to make the same mistakes in the final exam and their project. We cannot emphasize this enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuETOl5gQuNb"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Import necessary libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4us_KsqtN1Y"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Intermediate step to address fac that PCA is not scale-invariant\n",
    "\n",
    "\n",
    "# TO-DO: Instantiate and Fit PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeJnO6EENQQq"
   },
   "source": [
    "_**Cumulative Explained Variance Ratios**_\n",
    "\n",
    "Create an array of explained variance ratios and store it into a variable called `explained_variance_ratios`. Also, calculate the _cumulative_ explained variance ratios and store that into another variable called `cum_evr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_o9yURt1GLMs"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Save the explained variance ratios into variable called \"explained_variance_ratios\"\n",
    "\n",
    "\n",
    "# TO-DO: Save the CUMULATIVE explained variance ratios into variable called \"cum_evr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "synUBPNMGY7e"
   },
   "outputs": [],
   "source": [
    "#Grader cell (2 points)\n",
    "grader.grade(test_case_id = 'check_pca_explained_variance', answer = (explained_variance_ratios, cum_evr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNzvjLeTNZB-"
   },
   "source": [
    "Now plot the _cumulative_ `explained_variance_ratio` against the number of components to decide the number of components you should keep (this should look very similar to a visualization in lecture). Also add a horizontal line that represents the 80% of the variance as a threshold.\n",
    "\n",
    "As before, you should ensure that the plot follows the best practices you've developed over the past 2 plotting exercises in this HW (Labelling the axes and adding title to the plot, readability, etc.). **This will be manually graded for 2 points**\n",
    "\n",
    "_**CAUTION:** Recall that Python starts counting from 0, but this would make it very non-intuitive when reading from your graph. We strongly recommend that you modify the x-axis of your graph so that the **number of components begins at 1**. Otherwise, you may risk making silly mistakes in the subsequent sections._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpuM4rGYNZ56"
   },
   "outputs": [],
   "source": [
    "# TO-DO: find optimal num components to use (n) by plotting explained variance ratio (2 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIhrE2ILNmve"
   },
   "source": [
    "_**Final PCA**_\n",
    "\n",
    "Using your results above to help you decide the number of components to keep, choose a number (`n`) that explains **at least 80% of total variance** in the dataset. Then re-fit and transform your PCA instance on the training set using the number of components you decided.\n",
    "\n",
    "Remember that your PCA instance should be **fit** on the training set (`X_train`) but **only transform and not fit** on the test set (`X_test`).\n",
    "\n",
    "Call your transformed set of principal components `X_test_pca` in order to submit it to the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3jrh59gGZ0s"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Get transformed set of principal components on x_test\n",
    "\n",
    "# 1. Refit and transform on training with parameter n (as deduced from the last step)\n",
    "\n",
    "\n",
    "# 2. Transform on Testing Set and store it as `X_test_pca`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlIUmcW5Gl0M"
   },
   "outputs": [],
   "source": [
    "# Grader cell (2 points)\n",
    "grader.grade(test_case_id = 'check_fitted_pca', answer = X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzIlLTj0Nq92"
   },
   "source": [
    "#### (e) Logistic Regression with PCA [4 pts]\n",
    "\n",
    "1. Fit the logistic regression on your Final Principal Components data using your optimal `n`. Name the model `log_reg_pca`\n",
    "2. Make predictions on the **test** set and store this as `y_pred`.\n",
    "3. Report accuracy for the **test** set and call it `test_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5w5ZgkAGoXh"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Initialize `log_reg_pca` model with default parameters and fit it on the PCA transformed training set\n",
    "\n",
    "\n",
    "# TO-DO: Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`\n",
    "\n",
    "\n",
    "# TO-DO: Find the accuracy and store the value in `test_accuracy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVrpOF6NHDVX"
   },
   "outputs": [],
   "source": [
    "# Grader (4 points)\n",
    "grader.grade(test_case_id = 'check_log_reg_pca', answer = (test_accuracy, log_reg_pca.n_features_in_, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYGjFnJ8kfaK"
   },
   "source": [
    "### **1.4.3.0** Regression: Split Data into Train and Test [4 pts]\n",
    "\n",
    "\n",
    "**Task:**\n",
    " We will be predicting `avg_price_per_room` for regression models.\n",
    "\n",
    " Drop the columns: `'arrival_year'`,`arrival_month`, `'arrival_date'`,`'no_of_previous_cancellations'`, `'no_of_previous_bookings_not_canceled'` `'booking_status'`\n",
    " from `encoded_df_reservations` and save the dataframe to `reg_df_reservations`. Then, use scikit-learn's `train_test_split` function to split data for regression into train and test sets. The split should be 80-20 meaning 80% for training and rest for testing.\n",
    "\n",
    "\n",
    "**_IMPORTANT_: Please set the `seed` variable to 42 and then set the parameter to `random_state = seed`** and store the resulting splits as `X_train, X_test, y_train,` and `y_test`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzF_JNnEhvBC"
   },
   "outputs": [],
   "source": [
    "# TO-DO: drop arrival_year, arrival_month, arrival_date, no_of_previous_cancellations, no_of_previous_bookings_not_canceled,booking_status\n",
    "\n",
    "\n",
    "# TO-DO: save the dataframe to reg_df_reservations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f8a8uTcmTSV"
   },
   "outputs": [],
   "source": [
    "# TO-DO: stores features dataframe into variable called \"features\"\n",
    "\n",
    "\n",
    "# TO-DO: store the classification target variable  (`avg_price_per_room`) into \"target\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5rG25VRkfaL"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Assign appropriate value to seed and conduct 80/20 train-test split with random_state = seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfU3m6MNkfaL"
   },
   "outputs": [],
   "source": [
    "# Grader cell (4 points)\n",
    "grader.grade(test_case_id = 'train_test_split_regression', answer = (X_train.shape, X_test.shape,\n",
    "                                                                     y_train.shape, y_test.shape,\n",
    "                                                                     seed, len([i for i in y_test if i < 100])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGlb6tnNcKO-"
   },
   "source": [
    "### **1.4.3.1** Regression Models\n",
    "\n",
    "In this section, we will switch from classification models to regression models.\n",
    "\n",
    "Let's use the features we created in 1.4.3.0 to create regression models and predict the **average price per room**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS06q6k8UosE"
   },
   "source": [
    "#### (a) Linear Regression (Unregularized) [5 pts]\n",
    "\n",
    "Use the `LinearRegression` class in scikit-learn to perform Linear Regression. Initialize a Linear regression model named `reg` with default parameters, fit the model to the training set, and then make predictions on the testing set.\n",
    "\n",
    "Save your predictions in an array named `y_pred`, and report your R-squared score (saved it as a variable called `lin_reg_score`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PCTGcGEokY2"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Import required libraries\n",
    "\n",
    "\n",
    "# TO-DO: Initialize model with default parameters and fit it on the training set\n",
    "\n",
    "\n",
    "# TO-DO: Use the model to predict on the test set and save these predictions as `y_pred`\n",
    "\n",
    "\n",
    "# TO-DO: Find the R-squared score and store the value in `lin_reg_score`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCABLR7TokV4"
   },
   "outputs": [],
   "source": [
    "# Grader (5 points)\n",
    "grader.grade(test_case_id = 'check_linear_reg', answer = (lin_reg_score, reg.coef_, reg.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7EV4dpDcKO_"
   },
   "source": [
    "#### (b) Ridge Regression [2 pts]\n",
    "\n",
    "Use the `Ridge` class in scikit-learn to perform $L_2$ Regularized Linear Regression. Initialize a Ridge regression model named `reg_ridge` with regularization strength `alpha = 10`, fit the model to the training set, and then make predictions on the testing set.\n",
    "\n",
    "**Note:** Recall that Ridge regression is not scale-invariant, so you will need to standardize the features prior to modeling.\n",
    "\n",
    "Report your $R^2$ score and save it as a variable called `ridge_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUi11tA3zCUe"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Intermediate step to address scale-invariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tng1e7myy9gn"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Import required libraries\n",
    "\n",
    "\n",
    "# TO-DO: Initialize model with alpha = 10 (keep other hyperparameters as default values) and fit it on the training set\n",
    "\n",
    "\n",
    "# TO-DO: Use the model to predict on the test set and save these predictions as `y_pred`\n",
    "\n",
    "\n",
    "# TO-DO: Find the R-squared score and store the value in `ridge_score`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytmTMU7Z0jxo"
   },
   "outputs": [],
   "source": [
    "# Grader (2 points)\n",
    "grader.grade(test_case_id = 'check_ridge_reg', answer = (ridge_score, reg_ridge.alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1Cl9DePQzfq"
   },
   "source": [
    "_**Exam-Style Practice Questions:**_\n",
    "\n",
    "_(While we will NOT grade this for the purpose of the HW, it is in your best interest to take just 30-60 seconds to think about this as I may very well put something similar in the final exam!_ ðŸ˜‰_)_\n",
    "\n",
    "- What do you notice about the $R^2$ scores of the Ridge and unregularized Linear Regression above?\n",
    "- If Ridge is supposed to \"improve\" Linear Regression, we may find it unusual if that is not happening here. Under what circumstances would we consider using Ridge over unregularized Linear Regression?\n",
    "- Would you expect different results if we made `alpha` bigger ($\\alpha \\rightarrow \\infty$)? smaller ($\\alpha \\rightarrow 0$)? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRb_Kzn1pZBD"
   },
   "source": [
    "#### (c) Random Forest Regression [3 pts]\n",
    "\n",
    "From the results of linear regression and ridge linear regression, we see a linear model may not be ideal for this regression problem. We want a model that can capture the complex relationship between the target variable and the features. Fortunately, we have access to ensemble methods, which combine multiple simple models to improve performance. We will explore the bagging algorithm called Random Forest Regressor using scikit-learn.\n",
    "\n",
    "Use the `RandomForestRegressor` class in Scikit-learn's ensemble library to perform Random Forest Regression. Initialize a Random Forest regression model named `reg_rf`, fit the model to the training set, and then make predictions on the testing set.\n",
    "\n",
    "Report your R-squared score and save it as a variable called `rfr_score`, set random state to 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSqhgdjqokTa"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Import required libraries\n",
    "\n",
    "\n",
    "# TO-DO: Initialize model with default parameters and fit it on the training set\n",
    "\n",
    "\n",
    "# TO-DO: Use the model to predict on the test set and save these predictions as `y_pred`\n",
    "\n",
    "\n",
    "# TO-DO: Find the R-squared score and store the value in `rfr_score`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkJoI5QTpID8"
   },
   "outputs": [],
   "source": [
    "# Grader (3 points)\n",
    "\n",
    "grader.grade(test_case_id = 'check_rf_reg', answer = (rfr_score, reg_rf.feature_names_in_, reg_rf.n_outputs_, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-f0WCnTowlu8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iX1UkLEjk7U_"
   },
   "source": [
    "### **1.4.4** K-Means Clustering\n",
    "\n",
    "K-means clustering is a popular unsupervised machine learning algorithm that is used for grouping similar data points into a predetermined number of clusters. It is a simple and effective algorithm that is widely used in various fields such as image processing, recommendation systems, and customer segmentation.\n",
    "\n",
    "An elbow plot is a visualization tool used to determine the optimal number of clusters for a dataset. It plots the within-cluster sum of squares (WCSS) against the number of clusters. WCSS is the sum of the squared distance between each data point and its assigned centroid. The elbow plot helps you to choose the number of clusters that minimize the WCSS while avoiding overfitting.\n",
    "\n",
    "Please refer to [this document](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) to find out how to compute the sum of squared distances\n",
    "\n",
    "**We will be using the same features from the <ins>classification</ins> setting.\n",
    "To determine the best $k$ for the clustering, let's firstly generate an elbow plot.**\n",
    "- Consider number of clusters from 2 to 10\n",
    "- Set parameter `n_init` to 5.\n",
    "- Set random seed to 0\n",
    "\n",
    "**The elbow plot (4 points) will be manully graded**\n",
    "- x-axis is the \"number of clusters\", i.e. \"k\".\n",
    "- y-axis is the within-cluster sum of squares (WCSS) value\n",
    "- add proper titles for the plot and axis.\n",
    "\n",
    "Note that standardizing the data before performing k-means clustering is often recommended, but for illustration purpose, we do not standardize the input in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTF8RcWHtJf1"
   },
   "source": [
    "#### (a) Find the best number of clusters with the elbow plot [4 + 3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXEC7Ab9k_4W"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Prepare the data (the same features from the classification setting, i.e. excluding \"booking_status\")\n",
    "# stores features dataframe into variable called \"features\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frRJFXs9m-NT"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "\n",
    "# TO-DO: [MANUALLY GRADED: 4 points]\n",
    "#   for k ranges from 2 to 10, fit on \"features\" to generate an elbow plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMZqEOq7rW95"
   },
   "outputs": [],
   "source": [
    "# TO-DO: choose the best number of clusters (the elbow) and fill in\n",
    "number_of_cluster =\n",
    "\n",
    "# TO-DO: fill in the sum of squared distances for the best number of clusters\n",
    "wcss_elbow ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAVyIyXssIuf"
   },
   "outputs": [],
   "source": [
    "# Grader (3 points)\n",
    "grader.grade(test_case_id = 'kmeans_elbow', answer = (number_of_cluster, wcss_elbow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQLHFhzds7BC"
   },
   "source": [
    "#### (b) Re-fit with the best number of clusters [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUcjqzqxszye"
   },
   "outputs": [],
   "source": [
    "# TO-DO: re-run the K-Means clustering with the best number of clusters, save the fitted model to `kmeans`\n",
    "kmeans ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vF9gZHL6tkuo"
   },
   "outputs": [],
   "source": [
    "# Grader (3 points)\n",
    "grader.grade(test_case_id = 'refit_kmeans', answer = (Counter(kmeans.labels_), kmeans.n_features_in_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyURjOkMcKPD"
   },
   "source": [
    "# **Part II:** Distributed Machine Learning with Spark (35  points)\n",
    "\n",
    "Apache Spark ML is a machine learning library that consists of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction, and underlying optimization primitives.\n",
    "\n",
    "**Why Spark ML?**\n",
    "\n",
    "Moving to the Big Data Era requires heavy iterative computations on very large datasets. Standard implementations of machine learning algorithms require very powerful machines to be able to run. However, depending on high-end machines is not advantageous due to their high price and improper costs of scaling up. The idea of using distributed computing engines is to distribute the calculations to multiple low-end machines (commodity hardware) instead of a single high-end one. _This **definitely speeds up** the learning phase and allows us to create better models._\n",
    "\n",
    "Read more about it with the python documentation **[here](https://spark.apache.org/docs/latest/ml-guide.html)**.\n",
    "\n",
    "Run the code below to set up Spark.\n",
    "\n",
    "## _PENALTY WARNING_:\n",
    "We will very carefully go through the code in your notebook for this HW. Any student caught trying anything devious like using sklearn to avoid using SparkML in this section will get a **ZERO**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PY9ShBxkOQT0"
   },
   "source": [
    "**Note:** The cell below may take upto **4-5 minutes** to run so be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Eo8fwt8OcMa"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!apt install libkrb5-dev\n",
    "!wget https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
    "!pip install findspark\n",
    "!pip install sparkmagic\n",
    "!pip install pyspark\n",
    "! pip install pyspark --user\n",
    "! pip install seaborn --user\n",
    "! pip install plotly --user\n",
    "! pip install imageio --user\n",
    "! pip install folium --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwqTpkc3Ogz3"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName('bigdata-hw4').getOrCreate()\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAjmBWz7Oqs0"
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfQ72cpUOr3l"
   },
   "outputs": [],
   "source": [
    "# Graph section\n",
    "import networkx as nx\n",
    "\n",
    "# SQLite RDBMS\n",
    "import sqlite3\n",
    "\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = '/content/spark-3.1.2-bin-hadoop3.2'\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6F4x_ia7RGyp"
   },
   "source": [
    "## **2.1** Initializing Spark Data [Total: 2 Points]\n",
    "\n",
    "We have the Spark setup ready, and we now need the data for our ML algorithms. We will use the data you processed in Part I, but in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYTG07Kq4E82"
   },
   "source": [
    "### **2.1.0** Converting the Pandas Dataframe into a Spark Dataframe\n",
    "\n",
    "Read the `encoded_df_reservations` that you created in Part I into a Spark dataframe (`sdf`) and name the SDF as `reservations_sdf`. \\\\\n",
    "**Note:** Just like Part I, the target variable (label) for this section will be `\"booking_status\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQtC3kw7rf1V"
   },
   "outputs": [],
   "source": [
    "#TO-DO: Read encoded_df_reservations into a Spark Dataframe called reservations_sdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywmJD03O0Zas"
   },
   "source": [
    "Print out the Dataframe Schema and verify the datatypes. If you did everything correctly so far, the schema should be:\n",
    "```\n",
    "root\n",
    " |-- no_of_adults: long (nullable = true)\n",
    " |-- no_of_children: long (nullable = true)\n",
    " |-- no_of_weekend_nights: long (nullable = true)\n",
    " |-- no_of_week_nights: long (nullable = true)\n",
    " |-- required_car_parking_space: long (nullable = true)\n",
    " |-- lead_time: long (nullable = true)\n",
    " |-- arrival_year: long (nullable = true)\n",
    " |-- arrival_month: long (nullable = true)\n",
    " |-- arrival_date: long (nullable = true)\n",
    " |-- repeated_guest: long (nullable = true)\n",
    " |-- no_of_previous_cancellations: long (nullable = true)\n",
    " |-- no_of_previous_bookings_not_canceled: long (nullable = true)\n",
    " |-- avg_price_per_room: double (nullable = true)\n",
    " |-- no_of_special_requests: long (nullable = true)\n",
    " |-- booking_status: double (nullable = true)\n",
    " |-- type_of_meal_plan_Meal Plan 1: long (nullable = true)\n",
    " |-- type_of_meal_plan_Meal Plan 2: long (nullable = true)\n",
    " |-- type_of_meal_plan_Meal Plan 3: long (nullable = true)\n",
    " |-- type_of_meal_plan_Not Selected: long (nullable = true)\n",
    " |-- room_type_reserved_Room_Type 1: long (nullable = true)\n",
    " |-- room_type_reserved_Room_Type 2: long (nullable = true)\n",
    " |-- room_type_reserved_Room_Type 3: long (nullable = true)\n",
    " |-- room_type_reserved_Room_Type 4: long (nullable = true)\n",
    " |-- room_type_reserved_Room_Type 5: long (nullable = true)\n",
    " |-- room_type_reserved_Room_Type 6: long (nullable = true)\n",
    " |-- room_type_reserved_Room_Type 7: long (nullable = true)\n",
    " |-- market_segment_type_Aviation: long (nullable = true)\n",
    " |-- market_segment_type_Complementary: long (nullable = true)\n",
    " |-- market_segment_type_Corporate: long (nullable = true)\n",
    " |-- market_segment_type_Offline: long (nullable = true)\n",
    " |-- market_segment_type_Online: long (nullable = true)\n",
    "```\n",
    "\n",
    "**Note:** Especially ensure that the target variable is of the datatype `double`. Cast it to the correct datatype, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URd9g_An0qVY"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Verify that the schema of your Spark dataframe matches the above\n",
    "#        Make sure to double-check that the target variable i.e. booking_status is of type \"double\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3WUlDEv46rH"
   },
   "source": [
    "### **2.1.1** Setting Up a VectorAssembler\n",
    "\n",
    "Great! We have the processed data now. For Spark ML, we need to create a feature column which has all features concatenated as a list, and a single column for labels (which we already have!)\n",
    "We will use `VectorAssembler()` to create a feature vector from all our features, and we will call this vectorized variable as â€œfeaturesâ€.\n",
    "\n",
    "First, list all column names in `reservations_sdf` and store them in a list variable called `all_columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ya0DE8ml2YPj"
   },
   "outputs": [],
   "source": [
    "# TO-DO: store all column name in the data_sdf in a list called \"all_columns\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAMDrTU157iA"
   },
   "source": [
    "Create a list of column(s) you don't want to include as your features. Name this list `drop_columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbcp9QqH5_l0"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Create \"drop_columns\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j56ev2Mr6MZN"
   },
   "source": [
    "Now, using `drop_columns`, create a list called `feature_columns` that only contains the names of the columns that would be used as features for our ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMcrRpJb6Xo-"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Create the \"feature_columns\" list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5_jVWBI6znW"
   },
   "outputs": [],
   "source": [
    "# Grader cell [2 points]\n",
    "grader.grade(test_case_id = 'check_feature_columns', answer = feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFDTSMCM72uR"
   },
   "source": [
    "Finally, create a [VectorAssembler](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html) object with the columns that you want to use as features. Name your output column as `features` (i.e. they are the features that will be used for SparkML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmAZT38o7zxM"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Import required library and create VectorAssembler object (to be used later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYgt35u-68XA"
   },
   "source": [
    "## **2.2** Preprocessing: Pipeline and Train-Test Split [Total: 3 Points]\n",
    "\n",
    "Now that we have a VectorAssembler object ready, let's use it on our Spark dataframe to get the concatenated features column. To do this, we implement a [Pipeline](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html), which can be used to streamline multiple preprocessing stages at once. For this data, we just need a single stage with the assembler, but you could have other stages before that where you perform operations on the data like converting categorical strings in the features to numeric values, or do feature scaling operations.\n",
    "\n",
    "We will create a pipeline with a single stage â€” the assembler. Fit the pipeline on `reservations_sdf` to create the transformed dataframe and name it `processed_reservations_sdf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PozlPEwbMw9l"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Import required libraries\n",
    "\n",
    "\n",
    "# TO-DO: Define a pipeline object\n",
    "\n",
    "\n",
    "# TO-DO: Fit and transform the pipeline on the data and store the transformed sdf as \"processed_reservations_sdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LUua0RlNLpk"
   },
   "source": [
    "Now that we have the data in the format we need, we will create our train and test sets.\n",
    "\n",
    "Conduct a train-test split where 80% of the data is assigned to the training set while the remaining 20% is assigned to the testing set.\n",
    "\n",
    "- Name these sets as `train_sdf` and `test_sdf` respectively.\n",
    "- Set the variable `random_seed` to 42 and then add the argument `seed = random_seed` in the function to fix the random state in order to ensure consistency with our results.\n",
    "\n",
    "Note: Fit the pipeline on reservations_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXym9xONNrzB"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Assign appropriate value to the random_seed variable\n",
    "\n",
    "\n",
    "# TO-DO: Do 80/20 train-test split with seed = random_seed and store them as \"train_sdf\" and \"test_sdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNPtpqJyNuWA"
   },
   "outputs": [],
   "source": [
    "# Grader (3 points)\n",
    "grader.grade(test_case_id = 'check_train_test_split_spark', answer = (train_sdf.count(), test_sdf.count(), random_seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlAv_PlBcYDr"
   },
   "source": [
    "## **2.3** Modeling (SparkML) [Total: 30 Points]\n",
    "\n",
    "Time to do the cool stuff! We will work on various types of models using SparkML to see the difference in implementation from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0s65E3utg0Zo"
   },
   "source": [
    "### **2.3.1** Vanilla Logistic Regression [6 pts]\n",
    "\n",
    "Let's train an **unregularized** Logistic Regression model to our data and try to predict whether the `booking_status` will be cancelled or not.\n",
    "\n",
    "This time, we will use \"Big\" data tools to help us build a Logistic Regression model for binary classification. Look up the documentation online and try to understand how to implement this.\n",
    "\n",
    "\n",
    "**Note**: Make sure to set the parameter `maxIter` to an appropriate value (e.g. 5) for fast computation.\n",
    "\n",
    "**Checks:**\n",
    "\n",
    "- Are you sure you're using the correct evaluation metric? Always read the Official Documentation to see if the default parameters are what you expect. This is a recurring theme for many issues.\n",
    "\n",
    "- Did you make sure to order the inputs to the confusionMatrix function correctly so that it returns the confusion matrix in the form [[TP, FN], [FP, TN]]\n",
    "\n",
    "- Have you made sure to convert the booking_status column to type double? If you are facing a Py4JJavaErrors error, be sure to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IktoDMkWhn9p"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Import required library for Logistic Regression\n",
    "\n",
    "\n",
    "# TO-DO: Instantiate and fit Logistic Regression model to training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQWTQGRjiy2A"
   },
   "source": [
    "Calculate the training accuracy using the model's summary and store it in a variable called `train_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjyYCYw6i1sD"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Get training accuracy and store it as `train_accuracy`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C20Hy2qii_OO"
   },
   "source": [
    "Now, let's find out how good the model actually is and see if it overfits to the training data.\n",
    "\n",
    "Predict the labels `\"booking_status\"` for your test data and store them as `predictions` _(Hint: it is called 'transform' in SparkML)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szq8zQ3QjPWp"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Make predictions on testing set and store it as \"predictions\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kj_g8Ii_9-Gm"
   },
   "source": [
    "To get the test accuracy, we will make use of a **confusion matrix**. It is used to consolidate the predictive performance of a model into a single table. In a binary classification scenario, it looks like this:\n",
    "<img src = \"https://s2.loli.net/2023/04/01/Bg4yaIRGkEZXxvD.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzEQ4ltcnKZg"
   },
   "source": [
    "Given the confusion matrix, we can calculate various metrics such as accuracy, precision, recall, F1 score etc.\n",
    "\n",
    "For our task, we will calculate the test accuracy. The formula for this is: \n",
    "\n",
    "> Accuracy = $\\frac{TP + TN}{TP + FP + TN + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBjkb5KAn2rl"
   },
   "source": [
    "Evaluate the performance using the Confusion Matrix using [MulticlassMetrics](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.evaluation.MulticlassMetrics.html) object in the SparkML Regression library. Store the Confusion Matrix as an numpy array named  `confusion_matrix`. \\\\\n",
    "\n",
    "(*Ignore any warnings you may get*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJ7JKag7jUMW"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Import required libraries\n",
    "\n",
    "\n",
    "# TO-DO: Select appropriate columns to use MulticlassMetrics\n",
    "\n",
    "\n",
    "# TO-DO: Instantiate metrics objects\n",
    "\n",
    "\n",
    "# TO-DO: Create confusion matrix and store it as a numpy array named \"confusion_matrix\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWDnRN0upGh1"
   },
   "source": [
    "Now, calculate the test accuracy using the Confusion Matrix obtained above and store it in a variable called `test_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3IRPWAEpDpO"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Calculate test accuracy using the confusion matrix and store it as \"test_accuracy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAcHDaDPqp4Z"
   },
   "outputs": [],
   "source": [
    "# Grader cell (6 points)\n",
    "grader.grade(test_case_id = 'check_unreg_logistic', answer = (train_accuracy, test_accuracy, confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu_wBe65VrW9"
   },
   "source": [
    "### **2.3.2** Regularized Logistic Regression [8 pts]\n",
    "\n",
    "Now, we will add regularization â€“ LASSO (L1), Ridge (L2) and elastic net (combination of L1 and L2), to avoid overfitting. You can play around with different regularization hyperparameters when initializing the 3 different regularized logistic regression models.\n",
    "\n",
    "`elasticNetParam` controls the type of parameterisation, which when at 0 and 1 denotes ridge regression and LASSO regression respectively and everything between a linear combination of both. `regParam` controls the degree of parametrisation (defaults to 1.0). More mathematical details can be found here: https://runawayhorse001.github.io/LearningApacheSpark/reg.html. (Hint: choose your `regParam` wisely)\n",
    "\n",
    "Compare these with each other and with the unregularized regression performed in 2.3.1.\n",
    "\n",
    "Make sure to evaluate the performance on test data using the same methodology as above (using a confusion matrix to calculate the accuracy).\n",
    "\n",
    "**Note**: Just like 2.3.1, set `maxIter` to a suitable value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSEYNPEPYBJk"
   },
   "source": [
    "#### (a) LASSO (L1)\n",
    "\n",
    "First, implement a LASSO Logistic Regression model using SparkML and call it `l1_model`, fit it on `train_sdf` and get predictions using `test_sdf`. Get the training accuracy and store it as `l1_train_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlC3TNEnYzD2"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Instantiate LASSO/L1 regularized model as \"l1_model\"\n",
    "\n",
    "\n",
    "# TO-DO: Fit and Transform using \"l1_model\"\n",
    "\n",
    "\n",
    "# TO-DO: Store training accuracy as \"l1_train_accuracy\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZTFIUbicwmh"
   },
   "source": [
    "From the predictions, create a confusion matrix and use it to get the test accuracy. Store this accuracy value as `l1_test_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXTGezetajEi"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Create the Confusion matrix\n",
    "\n",
    "\n",
    "# TO-DO: Calculate the test accuracy as done in the previous section. Store test accuracy as \"l1_test_accuracy\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-O7Oq8KPdD9h"
   },
   "source": [
    "#### (b) Ridge (L2)\n",
    "\n",
    "Next, implement a Ridge Logistic Regression model using SparkML and call it `l2_model`, fit it on `train_sdf` and get predictions using `test_sdf`. Get the training accuracy and store it as `l2_train_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzxfiooYdO-s"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Instantiate Ridge/L2 regularized model as \"l2_model\"\n",
    "\n",
    "\n",
    "# TO-DO: Fit and Transform using \"l2_model\"\n",
    "\n",
    "\n",
    "# TO-DO: Store training accuracy as \"l2_train_accuracy\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6BD1lsvdjZS"
   },
   "source": [
    "From the predictions, create a confusion matrix and use it to get the test accuracy. Store this accuracy value as `l2_test_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vron91efdi8s"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Create the Confusion matrix\n",
    "\n",
    "\n",
    "# TO-DO: Calculate the test accuracy as done in the previous section. Store test accuracy as \"l2_test_accuracy\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUibpxvqeCsz"
   },
   "source": [
    "#### (c) Elastic Net\n",
    "\n",
    "Finally, implement an Elastic Net Logistic Regression model using SparkML and call it `en_model`, fit it on `train_sdf` and get predictions using `test_sdf`. Get the training accuracy and store it as `en_train_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_j1zU38eSAS"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Instantiate Elastic Net regularized model as \"en_model\"\n",
    "\n",
    "\n",
    "# TO-DO: Fit and Transform using \"en_model\"\n",
    "\n",
    "\n",
    "# TO-DO: Store training accuracy as \"en_train_accuracy\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xge3cFxecaU"
   },
   "source": [
    "From the predictions, create a confusion matrix and use it to get the test accuracy. Store this accuracy value as `en_test_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oI6OHfWJeeeX"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Create the Confusion matrix\n",
    "\n",
    "\n",
    "# TO-DO: Calculate the test accuracy as done in the previous section. Store test accuracy as \"en_test_accuracy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RU3LaLu6ewjQ"
   },
   "outputs": [],
   "source": [
    "#8 points\n",
    "#Grader cell\n",
    "grader.grade(test_case_id = 'check_reg_logistic', answer = (l1_model.getElasticNetParam(), l1_model.getRegParam(), l1_train_accuracy, l1_test_accuracy,\n",
    "                                                            l2_model.getElasticNetParam(), l2_model.getRegParam(), l2_train_accuracy, l2_test_accuracy,\n",
    "                                                            en_model.getElasticNetParam(), en_model.getRegParam(), en_train_accuracy, en_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8k-KqujfWbA"
   },
   "source": [
    "### **2.3.3** Random Forest Classification [8 pts]\n",
    "\n",
    "If you are looking to win Kaggle competitions, you definitely must know about Random Forests, Boosted Trees, etc. These ensemble methods generalize and work surprisingly well for a lot of classification problems and sometimes for regression problems (though regression trees are not the focus of this course).\n",
    "\n",
    "Let's give it a go. Similar to what you did for the Logistic Regression model, create a Random Forest classifier model and name it `rf` and fit it to the training data. Call the fitted model `rf_model`.\n",
    "\n",
    "**Note1:** Set the `random_seed` to 42 and set the argument `seed = random_seed` while creating `rf` as a RandomForest object. \\\\\n",
    "**Note2:** Set `maxDepth` to a reasonable value (e.g. 10) to find a good balance between performance and computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-rKH7Qh0m4H"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Import required libraries\n",
    "\n",
    "\n",
    "# TO-DO: Set random_seed to 42\n",
    "\n",
    "\n",
    "# TO-DO: Instantiate the RF Model and call it \"rf\", then fit it on the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M71HnmZ41eIy"
   },
   "source": [
    "This time, get predictions on both the training and testing sets, and store them as `train_pred` and `test_pred` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzPe3ld01hhA"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Get predictions and save to \"train_pred\" and \"test_pred\" respectively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcEQUtzP1tZ4"
   },
   "source": [
    "Evaluate using the accuracy metric again, similar to the previous sections.\n",
    "- Store the confusion matrices as `rf_train_cm` and `rf_test_cm` respectively. Note that the accuracies should be manually computed based on your confusion matrix.\n",
    "- Store the training and test accuracy as `rf_train_accuracy` and  `rf_test_accuracy` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "es9Wql3B14TV"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Evaluate the prediction accuracy for train set and call it \"rf_train_accuracy\"\n",
    "\n",
    "\n",
    "# TO-DO: Evaluate the prediction accuracy for test set and call it \"rf_test_accuracy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VH0SRIs3m30"
   },
   "outputs": [],
   "source": [
    "# Grader (8 points)\n",
    "grader.grade(test_case_id = 'check_random_forest', answer = (rf_train_accuracy, train_pred.count(), rf_test_accuracy, test_pred.count(), rf_train_cm, rf_test_cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n972zXod4DXF"
   },
   "source": [
    "### **2.3.4** Dimensionality Reduction Using PCA [8 pts]\n",
    "\n",
    "We will again use the powerful PCA to reduce the dimensions and project the data onto a lower dimensional space and fit a logistic regression on the new projected data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiL8evmIBhDL"
   },
   "source": [
    "Initialize a PCA model on SparkML, where you select an appropriate number of dimensions, and call it `pca`. Then, fit the model on the training data and get the PCA features from the trained model.\n",
    "\n",
    "**Hint**: Choose `k` for the PCA model using the explained variance plot you made earlier.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- Notice we're in SparkML world here, not sklearn! Check the Official Documentation for SparkML's StandardScaler to verify if the default parameters are what we would expect.\n",
    "\n",
    "- Also note that SparkML's scaling works slightly differs from sklearn's. The following link should help show and explain why we needed to configure withMean and withStd parameters a certain way for SparkML.\n",
    "\n",
    "- Make sure you are passing the correct inputs for Logistic Regression.\n",
    "\n",
    "- You may refer to these Stack Overflow posts: [[Link 1]](https://stackoverflow.com/questions/47770674/pca-output-in-spark-doesnt-matches-with-scikit-learn)[[Link 2]](https://stackoverflow.com/questions/57349987/standardscaler-difference-between-with-std-false-or-true-and-with-mean-false) highlighting the differences between using PCA and Standard Scaler in Scikit-learn vs. PySpark ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4t4b6RECbRl"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Import required libraries\n",
    "\n",
    "\n",
    "# TO-DO: Perform intermediate steps to address scale-variance in PCA\n",
    "\n",
    "\n",
    "# TO-DO: Instantiate PCA object as \"pca\" and then fit and transform.\n",
    "#        Make sure you make a reasonable choice for the number of Principal Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFxyRkmTDBta"
   },
   "source": [
    "Now, create a Logistic Regression model and train it using the PCA features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdX0LisYDcb1"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Instantiate Logistic Regression model and call the model object \"lr_model\"\n",
    "\n",
    "\n",
    "# TO-DO: Fit Logistic Regression Model and get predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz2c4099D_h-"
   },
   "source": [
    "Calculate the training accuracy using the model's summary and store it in a variable called `train_accuracy_pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EafeNRLMD7Rh"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Get training accuracy and store it as `train_accuracy_pca`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRTBu6AOEe-L"
   },
   "source": [
    "\n",
    "Evaulate the test performance by creating a confusion matrix and call it `confusion_matrix_pca`. Then, calculate the test accuracy using the confusion matrix  and store it as `test_accuracy_pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AbNje50EU2a"
   },
   "outputs": [],
   "source": [
    "# TO-DO: Create confusion matrix and store it as \"confusion_matrix_pca\"\n",
    "\n",
    "\n",
    "# TO-DO: Calculate test accuracy and store it as \"test_accuracy_pca\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ighJ-OTRG1al"
   },
   "outputs": [],
   "source": [
    "# Grader cell (8 points)\n",
    "grader.grade(test_case_id = 'check_pca', answer = (pca.getK(), len(lr_model.coefficients), train_accuracy_pca, test_accuracy_pca, confusion_matrix_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfH7LMHMGpgB"
   },
   "source": [
    "# Homework Submission Reminders & Penalties\n",
    "\n",
    "Good job! You have finished the homework. The submission instructions are as follows:\n",
    "\n",
    "* **Double check** that you have the correct PennID (all numbers) in the autograder.\n",
    "\n",
    "* **Triple check** that you have all plots shown in this Colab notebook before submitting (otherwise, your submission will be subjected to a **penalty of -5 points**).\n",
    "\n",
    "*  Go to the \"File\" tab at the top left of the Colab UI, click \"Download .ipynb\" and then \"Download .py\".  **Rename** these two files to `homework4.ipynb` and `homework4.py` respectively and upload them to Gradescope.\n",
    "  - <ins>**WAIT UNTIL THE GRADESCOPE AUTOGRADER FINISHES RUNNING!**</ins> If we have to manually upload this for you after the deadline, **we will apply a penalty of -10 points** (this stacks with any other penatlies)\n",
    "\n",
    "* After the autograder finishes running, Remember to click on the \"Code\" tab and see what your `.ipynb` file looks like on Gradescope.\n",
    "  - Just like in HW2, if \"Large File Hidden\" shows up, you MUST fix it or ensure that you submit screenshots of your plots along with the code cell that generated it\n",
    "\n",
    "* Post any issues with submission on Ed and make sure to keep in mind the late day policy.\n",
    "\n",
    "* After you submit your code, the teaching staff will manually grade your Colab notebook in order to validate the correctness of your code."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1UWVvQSa1jqgLX7YYnDYNX0vd_q6OcKRT",
     "timestamp": 1679090520871
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
