{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlv8esDGMj0h"
   },
   "source": [
    "# CIS 5450 Homework 2: SQL\n",
    "## Due: Tuesday, February 18 2025, 11:59 PM EST\n",
    "### Worth 100 points in total (90 Points Autograded + 10 Points Manually Graded)\n",
    "\n",
    "Welcome to Homework 2! By now, you should be familiar with the world of data science and the Pandas library. This assignment focuses on helping you get to grips with a new tool: SQL.\n",
    "\n",
    "Through this homework, we will be working with SQL (specifically **pandasql**) by exploring a [Yelp](https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset/versions/6) dataset containing business, checkin, tip, and user data. We will also conduct some text analysis.\n",
    "\n",
    " <!-- We will finish off the homework with some text analysis. -->\n",
    "\n",
    "We are introducing a lot of new things in this homework, and this is often where students start to get lost. Thus, we **strongly** encourage you to review the slides/material as you work through this assignment.\n",
    "\n",
    "**Before you begin:**\n",
    "- Be sure to click \"Copy to Drive\" to make sure you're working on your own personal version of the homework\n",
    "- Check the **pinned FAQ post** on Ed for updates! If you have been stuck, chances are other students have also faced similar problems.\n",
    "- **WARNING:** You MUST check that your notebook displays ALL visualizations on the Gradescope preview AND verify that the autograder finishes running and gives you your expected score (not a 0). (Ed [#251](https://edstem.org/us/courses/44790/discussion/3426442)).\n",
    "  - **Penalty:** -10: if we have to resubmit your notebook to Gradescope for you after the deadline. (e.g. not naming your files correctly, not submitting `.py` and .`ipynb`, etc.).\n",
    "  - **Penalty:** -5: failing to restart and re-run your notebook and ensure that that all visualizations show up in the Gradescope preview of your `.ipynb` (see step 1 and 2 in screenshot below for how to check this) (e.g. `Large File Hidden Error`).\n",
    "  - **Note:** If your plot is not run or not present after we open your notebook, we will deduct the entire manually graded point value of the plot. (e.g. if your plot is worth 4 points, we will deduct 4 points).\n",
    "  - **Note:** If your `.py` file is hidden because it's too large, that's ok! We only care about your `.ipynb` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0hcZWDcqCUL"
   },
   "source": [
    "## Part 0: Libraries and Set Up Jargon (The usual wall of imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1726895173758,
     "user": {
      "displayName": "Shailesh Sridhar",
      "userId": "03315669653496159715"
     },
     "user_tz": 240
    },
    "id": "EOhd4iaEONEK",
    "outputId": "436d3236-a085-4008-d7cb-f5978e75b2ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HW_ID=cis5450o_fall24_HW2\n"
     ]
    }
   ],
   "source": [
    "%set_env HW_ID=cis5450o_spr25_HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brXQGz4QLIW2"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install penngrader-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48758,
     "status": "ok",
     "timestamp": 1726895022690,
     "user": {
      "displayName": "Shailesh Sridhar",
      "userId": "03315669653496159715"
     },
     "user_tz": 240
    },
    "id": "ylkUtozb2Oc8",
    "outputId": "9e589269-cda1-4873-a90f-4dbc1cafacb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy==1.4.46\n",
      "  Downloading SQLAlchemy-1.4.46-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy==1.4.46) (3.1.0)\n",
      "Downloading SQLAlchemy-1.4.46-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sqlalchemy\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.35\n",
      "    Uninstalling SQLAlchemy-2.0.35:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.35\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.46 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed sqlalchemy-1.4.46\n",
      "Collecting pandasql\n",
      "  Downloading pandasql-0.7.3.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pandasql) (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pandasql) (2.1.4)\n",
      "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.10/dist-packages (from pandasql) (1.4.46)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->pandasql) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandasql) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pandasql) (2024.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy->pandasql) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->pandasql) (1.16.0)\n",
      "Building wheels for collected packages: pandasql\n",
      "  Building wheel for pandasql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pandasql: filename=pandasql-0.7.3-py3-none-any.whl size=26772 sha256=d13207e5b242f97d9dfbfe4080505918b7a97df78df41ee451a9918bfd59d7b8\n",
      "  Stored in directory: /root/.cache/pip/wheels/e9/bc/3a/8434bdcccf5779e72894a9b24fecbdcaf97940607eaf4bcdf9\n",
      "Successfully built pandasql\n",
      "Installing collected packages: pandasql\n",
      "Successfully installed pandasql-0.7.3\n",
      "Requirement already satisfied: geopy in /usr/local/lib/python3.10/dist-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.10/dist-packages (from geopy) (2.0)\n",
      "Collecting kaleido\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kaleido\n",
      "Successfully installed kaleido-0.2.1\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy\n",
    "!pip install pandasql\n",
    "!pip install geopy\n",
    "!pip install -U kaleido\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAXVD0Lo454x"
   },
   "outputs": [],
   "source": [
    "from penngrader.grader import *\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import geopy.distance as gp\n",
    "import matplotlib.image as mpimg\n",
    "import plotly.express as px\n",
    "import pandasql as ps #SQL on Pandas Dataframe\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.dates import date2num\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TmMEXVcwONB"
   },
   "outputs": [],
   "source": [
    "# Five datasets we're using\n",
    "! wget -nc https://storage.googleapis.com/penn-cis5450/yelp_business.csv\n",
    "! wget -nc https://storage.googleapis.com/penn-cis5450/yelp_checkin.csv\n",
    "! wget -nc https://storage.googleapis.com/penn-cis5450/yelp_tip.csv\n",
    "! wget -nc https://storage.googleapis.com/penn-cis5450/yelp_user.csv\n",
    "! wget -nc https://storage.googleapis.com/penn-cis5450/hotel_reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5NogkmdhhQ7"
   },
   "outputs": [],
   "source": [
    "print(pd.__version__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeyWoMn6pxSC"
   },
   "source": [
    "### PennGrader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2DysIgZL2u-"
   },
   "outputs": [],
   "source": [
    "# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY.\n",
    "# IF NOT, THE AUTOGRADER WON'T KNOW WHO TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
    "# TODO: YOUR PENN-ID GOES HERE AS AN INTEGER\n",
    "STUDENT_ID = 99999999\n",
    "\n",
    "# You should also update this to a unique \"secret\" just for this homework, to\n",
    "# authenticate this is YOUR submission\n",
    "SECRET = STUDENT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNFSUjDSHlUY"
   },
   "source": [
    "Leave this cell as-is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STmv3XDuHko3"
   },
   "outputs": [],
   "source": [
    "%%writefile notebook-config.yaml\n",
    "\n",
    "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
    "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fja4mIB1L8oI"
   },
   "outputs": [],
   "source": [
    "grader = PennGrader('notebook-config.yaml', os.environ['HW_ID'], STUDENT_ID, SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w_ZD5UMfj26"
   },
   "source": [
    "# Yelp Dataset\n",
    "\n",
    "<br>\n",
    "<center><img src = \"https://static-prod.adweek.com/wp-content/uploads/2021/08/YelpLogoAugust2021.jpg\" width= \"500\" align =\"center\"/></center>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "I'm sure you have used Yelp when searching for good restaurants or reliable local services, and with good reason. In this homework, we'll explore Yelp's extensive datasets to gain valuable insights into businesses and users on the platform. The data we will be using includes:\n",
    "\n",
    "* Business: data about businesses listed on Yelp\n",
    "* Check-in: data about check-ins made on businesses\n",
    "* Tip: data about tips that users left for each business\n",
    "* User: data about each user on Yelp including friends and number of reviews\n",
    "\n",
    "We'll be parsing this data into dataframes and relations, and then exploring how to query and assemble the tables into results. We will primarily be using PandaSQL, but for some of the initial questions, we will ask you to perform the same operations in Pandas as well, so as to familiarize you with the differences and similarities of the two.\n",
    "\n",
    "For the final part of this homework, we'll perform some text analysis on a hotel review dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oENmSeFkFRCo"
   },
   "source": [
    "## Part 1: Load & Process our Datasets [12 points total]\n",
    "\n",
    "Before we get into the data, we first need to load and clean our datasets.\n",
    "\n",
    "# Metadata\n",
    "You'll be working with four CSV files:\n",
    "- `yelp_business.csv`\n",
    "- `yelp_checkin.csv`\n",
    "- `yelp_tip.csv`\n",
    "- `yelp_user.csv`\n",
    "\n",
    "The file `yelp_business.csv` contains data of businesses listed on Yelp such as name, address, stars, review count, etc.\n",
    "\n",
    "The file `yelp_checkin.csv` contains check-in data for businesses including day, hour, and number of people.\n",
    "\n",
    "The file `yelp_tip.csv` contains tips that each user left for each business.\n",
    "\n",
    "The file `yelp_user.csv` contains data of each user such as name, review count, friends, start date of membership, and etc.\n",
    "\n",
    "\n",
    "**TODO**:\n",
    "* Load `yelp_business.csv` and save the data to a dataframe called `business_df`.\n",
    "* Load `yelp_checkin.csv` and save the data to a dataframe called `checkin_df`.\n",
    "* Load `yelp_tip.csv` and save the data to a dataframe called `tip_df`.\n",
    "* Load `yelp_user.csv` and save the data to a dataframe called `user_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ipfWu0afpCz"
   },
   "outputs": [],
   "source": [
    "# TODO: Import the datasets to pandas dataframes -- make sure the dataframes are named correctly!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9z4IukuZpX3U"
   },
   "outputs": [],
   "source": [
    "# TODO: view business_df (just the first 5 rows with .head() is fine) to make sure the import was successful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zd7egLysp-BK"
   },
   "outputs": [],
   "source": [
    "# TODO: view checkin_df (just the first 5 rows with .head() is fine) to make sure the import was successful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNicxKmiqCic"
   },
   "outputs": [],
   "source": [
    "# TODO: view tip_df (just the first 5 rows with .head() is fine) to make sure the import was successful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_00COZ1oqGNq"
   },
   "outputs": [],
   "source": [
    "# TODO: view user_df (just the first 5 rows with .head() is fine) to make sure the import was successful\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgRgNCSJquoB"
   },
   "source": [
    "### 1.1 Data Preprocessing\n",
    "\n",
    "Next, we are going to want to clean up our dataframes. We will start with cleaning up `business_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEh-87BqyD5z"
   },
   "source": [
    "#### 1.1.1 Cleaning `business_df` [4 points]\n",
    "\n",
    "`.info()` gives us meaningful information regarding columns, their types, and the amount of nulls, based on which we can now clean our dataframe.\n",
    "\n",
    "Perform these steps and save results on a new dataframe: `business_cleaned_df`\n",
    "\n",
    "**TODO**:\n",
    "* Copy `business_df` to the new dataframe called `business_cleaned_df`\n",
    "* Remove extra quotation marks from the entries in the columns `name` and `address` in `business_cleaned_df`\n",
    "* Format column `categories` such that each row is a list of strings. \"Sporting Goods;Shopping\" -> ['Sporting Goods', 'Shopping']\n",
    "* Drop `neighborhood` column from `business_cleaned_df`\n",
    "* Create a column called `is_restaurant` where the value is 1 if `'Restaurants'` is in the list in column `categories` and is 0 otherwise\n",
    "* Create individual rows for each category of each business instead of having a list of categories (Hint: explode)\n",
    "* Sort `business_cleaned_df` by `business_id` and `categories` respectively in an ascending order and reset index as well as drop old indices\n",
    "* Create a list called `business_category_list` containing all unique values found in column `categories` and sort the list in an alphabetical order\n",
    "\n",
    "After performing these steps, `business_cleaned_df` should have the following schema:\n",
    "\n",
    "**Final Schema**:\n",
    ">business_id | name | address | city | state | postal_code | latitude | longitude | stars | review_count | is_open | categories | is_restaurant\n",
    ">--- | --- | --- |--- | --- | --- | --- |--- | --- | --- |--- |--- |---\n",
    "\n",
    "### **NOTES: MAKE SURE TO ALWAYS RESET INDEX. OTHERWISE, YOU MIGHT GET AN INTERNAL SERVER ERROR WHEN RUNNING THE TEST CASES!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxyBVlCLwWR4"
   },
   "outputs": [],
   "source": [
    "# View info of business_df\n",
    "business_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBBLxYaz2QKh"
   },
   "outputs": [],
   "source": [
    "# TODO: Save business_df to business_cleaned_df\n",
    "business_cleaned_df ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRhkZHBkq_sc"
   },
   "outputs": [],
   "source": [
    " # TODO: Remove extra quotations (â€œâ€) from name and address\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HURPCdPrCuu"
   },
   "outputs": [],
   "source": [
    "# TODO: Format column categories such that each row is a list of strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Trab6sxo1YX6"
   },
   "outputs": [],
   "source": [
    "# TODO: Drop neighborhood column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_1o_fZHJynx"
   },
   "outputs": [],
   "source": [
    "# TODO: Create a column called is_restaurant where the value is 1 if 'Restaurants' is within the list in the column categories and is 0 otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16XahuHoV8iD"
   },
   "outputs": [],
   "source": [
    "# TODO: Create individual rows for each category of each business (Hint: explode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qgg_aM2h1VLg"
   },
   "outputs": [],
   "source": [
    "# TODO: Sort business_id and categories by ascending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I029QC98IYrt"
   },
   "outputs": [],
   "source": [
    "# TODO: Create a list called business_category_list containing all unique values found in column categories and sort the list in an alphabetical order\n",
    "business_category_list ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXIx23EpWFTi"
   },
   "outputs": [],
   "source": [
    "# 4 points\n",
    "grader.grade(test_case_id = 'test_cleaning_business', answer = (len(business_category_list), business_cleaned_df.head(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Awewzgl3JYz"
   },
   "source": [
    "#### 1.1.2 Inspecting `checkin_df` [3 points]\n",
    "\n",
    "We will inspect `checkin_df` through the following steps:\n",
    "\n",
    "**TODO**:\n",
    "* Save `checkin_df` to the new dataframe called `checkin_cleaned_df`\n",
    "* Inspect the values in `weekday` column. Store the number of unique values for the column `weekday` in the variable called `weekday_count_checkin`\n",
    "* Inspect the values in `hour` column. Store the number of unique values for the column `hour` in the variable called `hour_count_checkin`\n",
    "* We would like to know the maximum number of total check-ins received by a business in the dataframe (the largest number of check-ins among all businesses). Store this value in the variable called `max_checkin_single`\n",
    "\n",
    "After performing these steps, `checkin_cleaned_df` should have the following schema (should be the same as `checkin_df`):\n",
    "\n",
    "**Final Schema**:\n",
    ">business_id | weekday | hour | checkins\n",
    ">--- | --- | --- |---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gnx3lUOr7Fhj"
   },
   "outputs": [],
   "source": [
    "# View info of checkin_df\n",
    "checkin_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXAmSuLt3Ztl"
   },
   "outputs": [],
   "source": [
    "# TODO: Save checkin_df to checkin_cleaned_df\n",
    "checkin_cleaned_df ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ns-lrDAj7YAo"
   },
   "outputs": [],
   "source": [
    "# TODO: Store the number of unique values for the column weekday in the variable called weekday_count_checkin\n",
    "weekday_count_checkin ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApV76mb430fR"
   },
   "outputs": [],
   "source": [
    "# TODO: Store the number of unique values for the column hour in the variable called hour_count_checkin\n",
    "hour_count_checkin ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBPBCQkk3lug"
   },
   "outputs": [],
   "source": [
    "# TODO: Store the maximum number of total check-ins for a single business in the variable called max_checkin_single\n",
    "max_checkin_single ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEMGzLbvg-qv"
   },
   "outputs": [],
   "source": [
    "# 3 points\n",
    "grader.grade(test_case_id = 'test_cleaning_checkin', answer = (weekday_count_checkin, hour_count_checkin, max_checkin_single))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "in0_6-2c3RMw"
   },
   "source": [
    "#### 1.1.3 Cleaning `tip_df` [2 points]\n",
    "\n",
    "We will clean `tip_df` through the following steps:\n",
    "\n",
    "**TODO**:\n",
    "* Save `tip_df` to the new dataframe called `tip_cleaned_df`\n",
    "* Drop all rows with NA\n",
    "* Convert the data type in `date` column from string to `datetime64[ns]` (Use default format: `'%Y-%m-%d'`)\n",
    "* Sort `tip_cleaned_df` by `business_id` and `user_id` in an ascending order and reset index\n",
    "\n",
    "After performing these steps, `tip_cleaned_df` should have the following schema (should be the same as `tip_df`):\n",
    "\n",
    "**Final Schema**:\n",
    ">text | date | likes | business_id | user_id\n",
    ">--- | --- | --- |--- |---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZ87eMU39B5q"
   },
   "outputs": [],
   "source": [
    "# View info of tip_df\n",
    "tip_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poCUWRqG87HD"
   },
   "outputs": [],
   "source": [
    "# TODO: Save tip_df to tip_cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjffdTXX89y5"
   },
   "outputs": [],
   "source": [
    "# TODO: Drop rows with NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTGj5K_289v_"
   },
   "outputs": [],
   "source": [
    "# TODO: Convert the data in date column from string to datetime64[ns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMXd21cJh7XB"
   },
   "outputs": [],
   "source": [
    "# TODO: Sort tip_cleaned_df by business_id and user_id in an ascending order and reset index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYPAMBN8hpWS"
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "grader.grade(test_case_id = 'test_cleaning_tip', answer = (tip_cleaned_df.head(5), len(tip_cleaned_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lOZKViO3RWp"
   },
   "source": [
    "#### 1.1.4 Cleaning `user_df` [3 points]\n",
    "\n",
    "To understand what each column means, please refer to [this documentation](https://www.yelp.com/dataset/documentation/main) (Note that the original datasets were in JSON format).\n",
    "\n",
    "Perform these steps and save results on a new dataframe: `user_cleaned_df`\n",
    "\n",
    "**TODO**:\n",
    "* Save `user_df` to the new dataframe called `user_cleaned_df`\n",
    "* Create a column called `has_friend` which contains a value of 1 if the user has at least 1 friends and contains a value of 0 otherwise\n",
    "* Drop unwanted columns so we only have the following columns in `user_cleaned_df`: `user_id, name, review_count, yelping_since, friends, elite, average_stars, has_friend`\n",
    "* Sort `user_cleaned_df` by `user_id` in an ascending order and reset index\n",
    "\n",
    "After performing these steps, `user_cleaned_df` should have the following schema:\n",
    "\n",
    "**Final Schema**:\n",
    ">user_id | name | review_count | yelping_since | friends | elite | average_stars | has_friend\n",
    ">--- | --- | --- |--- | --- | --- |--- |---\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "* Do not convert the `friends` column into a list of strings since the table is very large, and performing such an operation could lead to a depletion of your RAM. We will address this column later, but only on a smaller subset of the table.\n",
    "* `'friends'` column is of type string, and `'None'` should not be interpreted as `NA` but rather a string `'None'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDfP5zGpMt7O"
   },
   "outputs": [],
   "source": [
    "# View info of user_df\n",
    "user_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gM_Ri0dt891T"
   },
   "outputs": [],
   "source": [
    "# TODO: Save user_df to user_cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9FvvNaeUbot"
   },
   "outputs": [],
   "source": [
    "# TODO: Create a column called has_friend which contains a value of 1 if the user has friends and 0 otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSvrCzREbBRH"
   },
   "outputs": [],
   "source": [
    "# TODO: Drop unwanted columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iLZNXrKUvRR"
   },
   "outputs": [],
   "source": [
    "# TODO: Sort by user_id in an ascending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUy9fXMDk0q7"
   },
   "outputs": [],
   "source": [
    "# 3 points\n",
    "grader.grade(test_case_id = 'test_cleaning_user', answer = (user_cleaned_df.head(5), len(user_cleaned_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYp9fW_SvG3g"
   },
   "source": [
    "### 1.2 Your Sandbox\n",
    "\n",
    "`.info()` is just one of many basic tools that you can use for Exploratory Data Analysis (EDA). Instead of throwing you straight into the deep end, we wanted to give you a chance to take some time and explore the data on your own. **This section is not graded**, so for the speedrunners out there feel free to just jump in, but we wanted to at least give you a small space to utilize your EDA toolkit to familiarize yourself with all the data you just downloaded.\n",
    "\n",
    "Some suggestions to get you started:\n",
    "- `df.head()`\n",
    "- `df.describe()`\n",
    "- `Series.unique()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U429NTI5RY4a"
   },
   "outputs": [],
   "source": [
    "# Your EDA here! Feel free to add more cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLO0ZpVr54ZS"
   },
   "source": [
    "## Part 2: Exploring the Data with PandasSQL (and Pandas) [74 points total]\n",
    "\n",
    "Now that you are familiar (or still unfamiliar) with the dataset, we will now introduce you to SQL, or more specifically **pandasql**: a package created to allow users to query pandas DataFrames with SQL statements.\n",
    "\n",
    "**Notes: Please note that in this part, the questions are not necessarily ordered by increasing difficulty, i.e., the first question may not necessarily be the easiest, and the difficulty level may vary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_1-ZcAko9e-"
   },
   "source": [
    "## ðŸ‘‡ðŸ‘‡ðŸ‘‡ IMPORTANT: Pay VERY CLOSE attention to this style guide! ðŸ‘‡ðŸ‘‡ðŸ‘‡\n",
    "\n",
    "The typical flow to use pandasql (aliased as `ps`) is as follows:\n",
    "1. Write a SQL query in the form of a string\n",
    "    - **String Syntax:** use triple quotes `\"\"\"<your query>\"\"\"` to write multi-line strings\n",
    "    - **Aliases are your friend:** if there are very long table names or you find yourself needed to declare the source (common during join tasks), it's almost always optimal to alias your tables with short INTUITIVE alias names\n",
    "    - **New Clauses New Line:** each of the main SQL clauses (`SELECT`, `FROM`, `WHERE`, etc.) should begin on a new line\n",
    "    - **Use Indentation:** if there are many components for a single clause, separate them out with new <ins>indented</ins> lines.\n",
    "\n",
    "    Example below:\n",
    "    ```SQL\n",
    "    \"\"\"\n",
    "    SELECT ltn.some_id, SUM(stn.some_value) AS total\n",
    "    FROM long_table_name AS ltn\n",
    "         INNER JOIN short_table_name AS stn\n",
    "            ON ltn.common_key = stn.common_key\n",
    "         INNER JOIN med_table_name AS mtn\n",
    "            ON ltn.other_key = mtn.other_key\n",
    "    WHERE ltn.col1 > value\n",
    "         AND stn.col2 <= another_value\n",
    "         AND mtn.col3 != something_else\n",
    "    GROUP BY ltn.some_id\n",
    "    ORDER BY total\n",
    "    \"\"\"\n",
    "    ```\n",
    "2. Run the query using **ps.sqldf(your_query, locals())**\n",
    "\n",
    "Pandasql is convenient in that it allows you to reference the dataframes that are currently defined in your notebook, so you will be able to fully utilize the dataframes that you have created above!\n",
    "\n",
    "Given that it is a brand new language, we wanted to give you a chance to directly compare the similarities/differences of the pandas that you already know and the SQL that you are about to learn. Thus, for each of the simpler queries, we may ask that you **look into the question twice: once with pandas and once with pandasql**. The SQL queries may take a minute to run, don't worry that is normal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "All problems require the use of a single SQL query. If a query needs to be broken down into steps, make use of Common Table Expressions (CTEs): https://www.geeksforgeeks.org/cte-in-sql/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxy0N9pDZDE7"
   },
   "source": [
    "### 2.1 Location, Location, Location [22 points]\n",
    "\n",
    "In the world of real estate, the phrase \"location, location, location\" has long been a mantra, emphasizing the importance of where a property is situated. Surprisingly, this mantra holds true not just for real estate but also for the culinary world. When it comes to restaurants, location can make or break a business. In this analysis, we're going to take a deep dive into the intersection of restaurants and their locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PfvXCpC6NB2"
   },
   "source": [
    "#### 2.1.1 What are the cities with the most restaurants? [6 points]\n",
    "\n",
    "The dataframe `business_cleaned_df` contains information of each business. We want to know the top 10 cities with the most restaurants. Note that we can distinguish restaurants by their `business_id`.\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "* Use `pandas` to find top 10 cities in the `city` and `state` column with the most restaurants. Note that you should also include `state` in groupby since city names could repeat, i.e., `city` together with `state` are unique\n",
    "* Include average stars of restaurants in each city in the column called `average_stars`\n",
    "* Order the resulting dataframe by the total number of restaurants, `restaurant_count`, in descending order\n",
    "* Save the result as `most_res_df` and don't forget to reset index\n",
    "\n",
    "**Final Schema**:\n",
    ">city | state | restaurant_count | average_stars\n",
    ">--- | --- | --- |---\n",
    "\n",
    "**Hint:**\n",
    "* There are two indicators whether the business is a restaurant or not:\n",
    "    * `is_restaurant` equals `1`\n",
    "    * `categories` equals `'Restaurants'`\n",
    "* Think about how we cleaned `business_df` to figure out the best way to filter the restaurants! (Think about what explode, where we Hinted about using it, does, and whether the way you decide on results in counting duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYybPj-tbf6t"
   },
   "outputs": [],
   "source": [
    "# TODO: Use pandas to find top 10 cities (including state) with the most restaurants. Also include average stars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZA_b2J7a1j4"
   },
   "outputs": [],
   "source": [
    "# 3 points\n",
    "grader.grade(test_case_id = 'most_res_pd', answer = most_res_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WjJF8A3bl7W"
   },
   "source": [
    "**TODO:**\n",
    "* Now, use `pandasql` to find top 10 cities with the most restaurants\n",
    "* The result should be saved as `most_res_df_sql` and should be the same as the result from `pandas`\n",
    "\n",
    "**WARNING: DO NOT USE PANDAS FOR ALL SQL QUESTIONS! OTHERWISE, YOU WON'T RECEIVE CREDITS FOR ALL SQL QUESTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcHW1fe02LW_"
   },
   "outputs": [],
   "source": [
    "# TODO: Use SQL to find top 10 cities (including state) with the most restaurants. Also include average stars\n",
    "most_res_query = '''\n",
    "'''\n",
    "most_res_df_sql = ps.sqldf(most_res_query, locals())\n",
    "most_res_df_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cj_5REUcbrgd"
   },
   "outputs": [],
   "source": [
    "# 3 points\n",
    "grader.grade(test_case_id = 'most_res_sql', answer = (most_res_df_sql, most_res_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFa2sAULJNml"
   },
   "source": [
    "#### 2.1.2 Which cities have the highest average total check-ins per restaurant? [8 points]\n",
    "\n",
    "The dataframe `checkin_cleaned_df` contains check-in information for each business. We want to know the top 15 cities with the highest average check-ins per restaurant.\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "* Use `pandas` to find top 15 cities in the `city` and `state` column with the highest average check-ins per restaurant. Note that you should also include `state` in groupby since city names could repeat, i.e., `city` together with `state` are unique\n",
    "* Include average check-ins per restaurant in each city in the column called `avg_checkins`. Round the average check-ins to 1 decimal place\n",
    "* Order the resulting dataframe by `avg_checkins` in descending order\n",
    "* Save the result as `most_checkin_df` and don't forget to reset index\n",
    "\n",
    "**Notes:**\n",
    "* Only consider businesses with check-in data in `checkin_cleaned_df`. Do not include businesses with no check-in data in the average.\n",
    "* One `business_id` can appear in more than one row in `business_cleaned_df` and `checkin_cleaned_df`. Think about how we can filter restaurants from other types of business without taking duplicates.\n",
    "\n",
    "**Final Schema**:\n",
    ">city | state | avg_checkins\n",
    ">--- | --- | ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0YxVrMKhFxQ"
   },
   "outputs": [],
   "source": [
    "# TODO: Use pandas to find 15 cities (including state) with highest average total checkins per restaurant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foHQPnHKeUZE"
   },
   "outputs": [],
   "source": [
    "# 4 points\n",
    "grader.grade(test_case_id = 'most_checkin_pd', answer = most_checkin_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuWsyJiNhMl1"
   },
   "source": [
    "**TODO:**\n",
    "* Now, use `pandasql` to find top 15 cities with the highest average total check-ins per restaurant\n",
    "* The result should be saved as `most_res_df_sql` and should be the same as the result from `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7UItnNw_Mg8"
   },
   "outputs": [],
   "source": [
    "# TODO: Use SQL to find 15 cities (including state) with highest average total checkins per restaurant\n",
    "most_checkin_query = '''\n",
    "'''\n",
    "most_checkin_df_sql = ps.sqldf(most_checkin_query, locals())\n",
    "most_checkin_df_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSmvDc3xfPMk"
   },
   "outputs": [],
   "source": [
    "# 4 points\n",
    "grader.grade(test_case_id = 'most_checkin_sql', answer = (most_checkin_df_sql, most_checkin_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTZsT5z01eo9"
   },
   "source": [
    "#### 2.1.3 Which states have the most popular Thai restaurants? [8 points]\n",
    "We would like to know which state has the most popular Thai restaurants, which we will gauge using the number of reviews.\n",
    "\n",
    "**TODO:**\n",
    "* Use `business_cleaned_df` to find Thai restaurants with the highest `review_count` for each state\n",
    "* The resulting dataframe should include the `state` and `city` in which the restaurant is located, the `name` of the restaurant, the number of reviews in the `max_review` column, and the `stars` of the restaurant\n",
    "* If there are more than one Thai restaurants in the same state with the maximum number of reviews, include all of them in the resulting dataframe\n",
    "* Save the result as `popular_thai_df` and don't forget to reset index\n",
    "* Finally, order the resulting dataframe by `max_review` in a descending order and `name` in an ascending order respectively\n",
    "* Hint: Make sure you are only considering restaurants\n",
    "\n",
    "**Final Schema**:\n",
    ">state | city | name | max_review | stars\n",
    ">--- | --- | --- | --- | ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDtHj_4H5AYC"
   },
   "outputs": [],
   "source": [
    "# TODO: Use pandas to find Thai restaurants with highest review count for each state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4GIspvOhjZ_"
   },
   "outputs": [],
   "source": [
    "# 4 points\n",
    "grader.grade(test_case_id = 'popular_thai_pd', answer = popular_thai_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8MNpEyE41Y0"
   },
   "source": [
    "**TODO:**\n",
    "* Now, use `pandasql` to find Thai restaurants with highest review count for each state\n",
    "* The result should be saved as `popular_thai_df_sql` and should be the same as the result from `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tA7BsRmI3VaG"
   },
   "outputs": [],
   "source": [
    "# TODO: Use SQL to find Thai restaurants with highest review count for each state\n",
    "popular_thai_query = '''\n",
    "'''\n",
    "popular_thai_df_sql = ps.sqldf(popular_thai_query, locals())\n",
    "popular_thai_df_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSs5AQ-oifCo"
   },
   "outputs": [],
   "source": [
    "# 4 points\n",
    "grader.grade(test_case_id = 'popular_thai_sql', answer = (popular_thai_df_sql, popular_thai_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gVAIFiYMdpf"
   },
   "source": [
    "###2.2 Best Time to Visit Restaurant [14 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcFlnI8G9-T0"
   },
   "source": [
    "#### 2.2.1 Finding restaurants  [6 points]\n",
    "\n",
    "After working hard on this course, you were able to find a summer internship position in Phoenix, AZ. You want to explore food places and you decided to start with Italian restaurants. We will use `business_cleaned_df` to find the top 10 Italian restaurant in Phoenix, AZ that has the highest `review_count`.\n",
    "\n",
    "**TODO** (`pandas`):\n",
    "* Filter the `business_cleaned_df` on the given conditions.\n",
    "* Keep only `business_id`,`name`, `stars`, `review_count`, and `categories` for the final dataframe\n",
    "* Sort by the `review_count` descending order\n",
    "\n",
    "**Final Schema**:\n",
    ">business_id | name | stars | review_count | categories\n",
    ">--- | --- | --- | --- | ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZedXYZ3q_UQm"
   },
   "outputs": [],
   "source": [
    "# TODO: Finding restaurants\n",
    "best_italian_df ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFjh-3hOHlx1"
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "grader.grade(test_case_id = 'test_italian', answer = best_italian_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SY9roAN-KI32"
   },
   "source": [
    "**TO-DO** : Repeat the same using `business_cleaned_df` (only for this sql query) and `pandasql`, saving it as `best_italian_df_sql`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bb4EFj__Upx"
   },
   "outputs": [],
   "source": [
    "# TODO: Use SQL to find top 10 Italian restaurants with highest review count in Pheonix, AZ\n",
    "best_italian_df_query = '''\n",
    "'''\n",
    "best_italian_df_sql = ps.sqldf(best_italian_df_query, locals())\n",
    "best_italian_df_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3zXOF52FbHL"
   },
   "outputs": [],
   "source": [
    "# 4 points\n",
    "grader.grade(test_case_id = 'test_italian_sql', answer = (best_italian_df_query,best_italian_df_sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTA-d4G5FbyQ"
   },
   "source": [
    "#### 2.2.2 Avoid Restaurant Traffic  [8 points]\n",
    "\n",
    "You visited one of the Italian restaurants from the 2.2.1 list. But, you weren't able to find a table to seat because the restaurant was very crowded at the time of your visit. Now, you want to try Burgers instead and want to find a better time to visit so you don't have to wait.\n",
    "\n",
    "* Use `business_cleaned_df` and to find the top 5 Burgers restaurant by review counts in Phoenix, AZ.\n",
    "\n",
    "* `checkin_cleaned_df` will be used to find the number of check-ins at specific day and hour.\n",
    "\n",
    "* Keep only the rows that have less than 5 checkins at given day and hour\n",
    "\n",
    "**Final Schema**:\n",
    ">name | address | city | state | stars | review_count | categories | weekday | hour | checkins\n",
    ">--- | --- | --- | ---  | --- | --- | --- | --- | --- | ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFM3qeerFsXX"
   },
   "outputs": [],
   "source": [
    "# TODO: Avoid Restaurant Traffic\n",
    "best_burgers_df ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvRsM0zHHkyj"
   },
   "outputs": [],
   "source": [
    "# 3 points\n",
    "grader.grade(test_case_id = 'test_burgers', answer = best_burgers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5jZvklWZZDg"
   },
   "source": [
    "**TODO:**\n",
    "* Now, use `pandasql` to find the top 5 Burgers restaurants in Phoenix, AZ with less than 5 check-ins.\n",
    "* The result should be saved as `best_burgers_sql` and should be the same as the result from `pandas`\n",
    "* Note that you must use a single SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76KJayxiCL_5"
   },
   "outputs": [],
   "source": [
    "# TODO: Use SQL to find the number of checkins that are less than 5 for\n",
    "# top 5 Burgers restaurants with highest review count\n",
    "best_burgers_query = '''\n",
    "'''\n",
    "best_burgers_sql = ps.sqldf(best_burgers_query, locals())\n",
    "best_burgers_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwqmlLrBHj--"
   },
   "outputs": [],
   "source": [
    "# 5 points\n",
    "grader.grade(test_case_id = 'test_burgers_sql', answer = (best_burgers_query,best_burgers_sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcqNApWfXfI2"
   },
   "source": [
    "###2.3 Avid Yelpers [22 points]\n",
    "In this section we'll be taking a deeper dive into `user_cleaned_df`. We'll be focusing on Yelp users who are particularly active! In this digital age, even Yelp has a friends/follower count ðŸ˜². Let's see who's friends with who, and who reigns supreme on the newest social media platform -- Yelp!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HCFx2-2ZJfL"
   },
   "source": [
    "#### 2.3.1 Elite Yelp Reviewers [6 points]\n",
    "\n",
    "Use `user_cleaned_df` to find all elite Yelp users who have at least have one friend and have left at least 2000 reviews.\n",
    "\n",
    "**TODO:**\n",
    "* Filter for the above characteristics\n",
    "* Sort the dataframe by `review_count` in descending order, then by `yelping_since` in ascending order\n",
    "* Reset index\n",
    "* Save the resulting dataframe as `elite_user_df`\n",
    "\n",
    "**NOTE:**\n",
    "* \"Elite users\" are those who have year(s) listed in the `elite` column of `user_cleaned_df`. **Tip:** Use `.value_counts()` function to view possible values for the `elite` column and identify ways to effectively filter for these users.\n",
    "* Beware of the datatype of `elite` when dealing with not elite users\n",
    "\n",
    "**Final Schema**:\n",
    ">user_id | name | review_count | yelping_since | friends | elite | average_stars | has_friend |\n",
    ">--- | --- | --- | ---  | --- | --- | --- | --- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sx7DZosudeao"
   },
   "outputs": [],
   "source": [
    "# TODO: Use pandas to find elite yelp reviewers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzFIQ8BJZ7xH"
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "grader.grade(test_case_id = 'elite_user_pd', answer = (elite_user_df.head(), len(elite_user_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlRa7pBcZTTc"
   },
   "source": [
    "**TODO:**\n",
    "* Now, use `pandasql` to find all elite Yelp users who have at least have one friend and have left at least 2000 reviews.\n",
    "* The result should be saved as `elite_user_df_sql` and should be the same as the result from `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeyNrNqNZy9I"
   },
   "outputs": [],
   "source": [
    "## TODO: Use SQL to find elite yelp reviewers\n",
    "elite_user_query = '''\n",
    "'''\n",
    "elite_user_df_sql = ps.sqldf(elite_user_query, locals())\n",
    "elite_user_df_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWHC7dQZZ4V9"
   },
   "outputs": [],
   "source": [
    "# 4 points\n",
    "grader.grade(test_case_id = 'elite_user_sql', answer = (elite_user_df_sql.head(), elite_user_query, len(elite_user_df_sql)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsyB903yjVTC"
   },
   "source": [
    "#### 2.3.2 Joining Users' Friends  [8 points]\n",
    "\n",
    "We now want to find the friends of all the elite users. Before we do any analysis, let's clean up the `friends` column.\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "*   Use `elite_user_df` and create a new row for each friend an elite user has (hint split the strings then explode, make sure there is no trailing whitespace)\n",
    "*   This modified `elite_user_df` will be used for the rest of 2.3.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VuBKBT3kTAu"
   },
   "outputs": [],
   "source": [
    "# TODO: Explode the friends column of elite_user_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfvHA9vnaRIG"
   },
   "source": [
    "Now we will merge the friends' information in pandas.\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "* Use `elite_user_df` and `user_cleaned_df` to join the friends' `name` and `average_stars`\n",
    "\n",
    "* Keep the elite user's `user_id`, `name`, and `average_stars`. You can drop all other columns\n",
    "\n",
    "* Rename the friend columns to `user_id_friend`, `name_friend`, and `average_stars_friend` (hint: check out the parameters of merge)\n",
    "* Sort the dataframe by `user_id` ascending, then by `user_id_friend` ascending\n",
    "* Save the resulting dataframe as `friends_df`\n",
    "\n",
    "**Final Schema**:\n",
    ">user_id | name | average_stars | user_id_friend | name_friend | average_stars_friend |\n",
    ">--- | --- | --- | ---  | --- |--- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIS-6ru5mf46"
   },
   "outputs": [],
   "source": [
    "# TODO: Use pandas to join elite yelp reviewers' friends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjZOKl1-rxhn"
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "grader.grade(test_case_id = 'friends_join_pd', answer = (friends_df.head(10), len(friends_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoKbu1lpaXpM"
   },
   "source": [
    "**TODO:**\n",
    "* Now, use `pandasql` to merge elite users' friends' information\n",
    "* The result should be saved as `friends_df_sql` and should be the same as the result from `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4Gm6lX6aZ33"
   },
   "outputs": [],
   "source": [
    "# TODO: Use SQL to join elite yelp reviewers' friends\n",
    "friends_join_query = '''\n",
    "'''\n",
    "friends_df_sql = ps.sqldf(friends_join_query, locals())\n",
    "friends_df_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkwENZSGsKae"
   },
   "outputs": [],
   "source": [
    "# 4 points\n",
    "grader.grade(test_case_id = 'friends_join_sql', answer = (friends_df_sql.head(), friends_join_query, len(friends_df_sql)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lR1S_9NLaVA5"
   },
   "source": [
    "#### 2.3.3 Do Friends Think Alike?  [8 points]\n",
    "\n",
    "Now that we joined all of the elite users' friends, we want to compare their average ratings. Let's take a look at the difference between elite users' average stars and the average of all their friends' stars.\n",
    "\n",
    "**TODO:**\n",
    "* Use `friends_df` to calculate the difference between each elite user's `average_stars` and the average of all of their friends' `average_stars` (`average_stars` - `average_stars_friend`). Name this column `average_stars_diff`\n",
    "\n",
    "* Save the results in `stars_diff_df`. Keep only the `user_id` and `average_stars_diff` columns.\n",
    "\n",
    "**Final Schema**:\n",
    ">user_id | average_stars_diff |\n",
    ">--- | --- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwC9RNuZu6zC"
   },
   "outputs": [],
   "source": [
    "# TODO: Use pandas to calculate the difference in average_stars amongst a user and their friends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igCUulx6u9xw"
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "grader.grade(test_case_id = 'stars_diff_pd', answer = stars_diff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tjX3_RFvAUA"
   },
   "source": [
    "**TODO:**\n",
    "* Now, use `pandasql` to to calculate the difference in `average_stars` amongst a user and their friends\n",
    "* The result should be saved as `stars_diff_df_sql` and should be the same as the result from `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu0J5UjlvCP_"
   },
   "outputs": [],
   "source": [
    "# TODO: Use SQL to calculate the difference in average_stars amongst a user and their friends\n",
    "stars_diff_query = '''\n",
    "'''\n",
    "stars_diff_df_sql = ps.sqldf(stars_diff_query, locals())\n",
    "stars_diff_df_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkFjlcXKvE-a"
   },
   "outputs": [],
   "source": [
    "# 4 points\n",
    "grader.grade(test_case_id = 'stars_diff_sql', answer = (stars_diff_df_sql, stars_diff_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfKqHmW6zWil"
   },
   "source": [
    "###2.4 Restaurant Reviews [16 points]\n",
    "In this section we'll take a look at Yelp restaurant reviews. Who leaves these comments? What type of restaurants get the most reviews? Stay tuned.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byfPwZkM0lWI"
   },
   "source": [
    "#### 2.4.1 Are reviewers on average positive or negative?  [8 points]\n",
    "\n",
    "Do people who leave **restaurant** reviews tend to be on average pessimistic or optimistic? Critical or soft? Let's take a look at the `average_stars` of users who leave Yelp reviews.\n",
    "\n",
    "**TODO:**\n",
    "* Use `tip_cleaned_df`,  `business_cleaned_df`, and `user_cleaned_df` to calculate the average rating of all Yelp reviewers in the dataset who have left a tip/review on a restaurant.\n",
    "* Make sure each user only appears once in the dataframe.\n",
    "\n",
    "* Save the results in `average_rating_tip_df`. This will be a one column, one row dataframe across all users\n",
    "\n",
    "**Final Schema**:\n",
    ">average_stars |\n",
    ">--- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3IIaBWR1s1c"
   },
   "outputs": [],
   "source": [
    "# TODO: Use pandas to calculate the average ratings of users who leave Yelp reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vn3Rt3k1sAG"
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "grader.grade(test_case_id = 'average_rating_tip_pd', answer = average_rating_tip_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vzfOJUa3Kbs"
   },
   "source": [
    "**TODO:**\n",
    "* Now, use `pandasql` to to calculate the average ratings of users who leave Yelp reviews on restaurants\n",
    "* The result should be saved as `average_rating_tip_df_sql` and should be the same as the result from `pandas`\n",
    "\n",
    "(Don't worry this will take a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSpp8Zna3OIp"
   },
   "outputs": [],
   "source": [
    "# TODO: Use SQL to calculate the average ratings of users who leave Yelp reviews\n",
    "average_rating_tip_query = '''\n",
    "'''\n",
    "average_rating_tip_df_sql = ps.sqldf(average_rating_tip_query, locals())\n",
    "average_rating_tip_df_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUR32DuY3S6l"
   },
   "outputs": [],
   "source": [
    "# 4 points\n",
    "grader.grade(test_case_id = 'average_rating_tip_sql', answer = (average_rating_tip_df_sql, average_rating_tip_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukmSCzGb43Cb"
   },
   "source": [
    "#### 2.4.2 What type of restuarants get the most reviews in January?  [8 points]\n",
    "\n",
    "We want to find out which restaurant categories get the most reviews in January across all years.\n",
    "\n",
    "**TODO:**\n",
    "* Use `tip_cleaned_df` and `business_cleaned_df` to find the count of reviews for each category\n",
    "* Keep the categories that have more than 100 reviews\n",
    "* Sort the dataframe descending by count\n",
    "* Save the results in `category_tip_df`\n",
    "\n",
    "**Note:**\n",
    "* Some categoties with the restaurant tag may not seem related-- that's okay.\n",
    "* Take a look at [this documentation](https://docs.python.org/3/library/datetime.html) for help with datetime objects\n",
    "* Make sure you use the 'is_restaurant' column to filter out restaurants\n",
    "\n",
    "\n",
    "**Final Schema**:\n",
    ">categories |count |\n",
    ">--- |--- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2Bk-PFh_ikl"
   },
   "outputs": [],
   "source": [
    "# TODO: Use pandas to calculate the count of restaurant reviews in January by category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40luyvep_cNh"
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "grader.grade(test_case_id = 'category_tip_pd', answer = category_tip_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRDgpMFV_N1Q"
   },
   "source": [
    "**TODO:**\n",
    "* Now, use `pandasql` to to calculate the count of restaurant reviews in January\n",
    "* The result should be saved as `category_tip_df_sql` and should be the same as the result from `pandas`\n",
    "* Hint: Try using the strftime function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2b6DakK5MFi"
   },
   "outputs": [],
   "source": [
    "# TODO: Use SQL to calculate the count of reviews in January by category\n",
    "category_tip_query = '''\n",
    "'''\n",
    "category_tip_df_sql = ps.sqldf(category_tip_query, locals())\n",
    "category_tip_df_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E79qunXN_GrW"
   },
   "outputs": [],
   "source": [
    "# 4 points\n",
    "grader.grade(test_case_id = 'category_tip_sql', answer = (category_tip_df_sql, category_tip_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_X6JARTSI1E"
   },
   "source": [
    "## Part 3: Data Visualization [6 points total - manually graded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBs4i670SJmY"
   },
   "source": [
    "You've done great work so far! Now let's create a couple visualizations to illustrate data we might be interested in.\n",
    "\n",
    "\n",
    "This section will be **manually graded**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89ihkDOCSNE6"
   },
   "source": [
    "Begin by following the directions below to prepare the dataset for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cy0E743dwT64"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hg2NQLybwX1p"
   },
   "source": [
    "For this part we will be using ``checkin_cleaned_df``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8xc3DELSOC-"
   },
   "outputs": [],
   "source": [
    "# Step 1 - using datetime functions, create a column called 'Time of Day' that has the time\n",
    "# (Hour:Minute) seen in the hour column\n",
    "\n",
    "# HINT: The date should be of type %H:%M, and once you've converted to datetime\n",
    "# you can use the .time() function to get the time of day (NOTE: there are multiple ways to do this so do not worry if your approach is different)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VjP3z3qwODK"
   },
   "outputs": [],
   "source": [
    "# Step 2 - Using the 'weekday' column, create a column called 'Day' which has the day\n",
    "# associated with the date (Monday, Tuesday, ..., Sunday). You can discard the\n",
    "# 'weekday' column once you're done.\n",
    "\n",
    "# Hint: You can try to come up with a way to map an abbreviated word to its full word.\n",
    "\n",
    "# Mon represents Monday, Tue represents Tuesday, ... , Sun represents Sunday\n",
    "\n",
    "# Use a lambda function to apply the mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Negr3ekNwMS1"
   },
   "outputs": [],
   "source": [
    "# Step 3 - Make a dataframe that just contains 'business_id', 'checkins', 'Time of Day', and 'Day'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnuXupTuSSFR"
   },
   "source": [
    "Your dataset is now complete! For the first line plot, you will be visualizing the number of trips that occur during each hour of the day, for all 7 days. To do so, you will be creating 2 line charts using Seaborn (sns).\n",
    "\n",
    "### This is important: using only Matplotlib will NOT result in full credit\n",
    "\n",
    "It is crucial that **your first line graph** contains the following features:\n",
    "1. The X-axis should be labelled \"Time of Day\", is of type datetime and ranges from 00:00 (midnight) to 23:00 (11 pm)\n",
    "2. The Y-axis should be labelled \"Average Checkins\".\n",
    "3. There should be a title called \"Average Checkins of Restaurants for each Hour of Day\"\n",
    "4. There are 7 lines for each day of the week, clearly labeled and differentiated, based on **color** and **markers**.\n",
    "5. Again, must be completed using Seaborn (sns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mILxbpMOSVgq"
   },
   "outputs": [],
   "source": [
    "# First Plot:\n",
    "\n",
    "# Use the dataframe you generated from the previous step\n",
    "plot1_df =\n",
    "\n",
    "# Now, alter this dataframe to have 3 columns, IN THIS ORDER:\n",
    "# 'Day', 'Time of Day', 'Average Checkins'. Average Checkins is the mean checkins\n",
    "# for the particular hour of that day.\n",
    "\n",
    "# Now just plot the graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-LfoueMmy_Z"
   },
   "source": [
    "Now merge with `checkins2` with `business_cleaned_df` to find the average of checkins each hours of day (`Time of Day`) to visit the American restaurant. `checkins2` refers to the dataframe that you cleaned above to complete 3.1.\n",
    "\n",
    "For this part, you will need to clean `business_cleaned_df` to keep stores that are restaurant and category is American (New).\n",
    "\n",
    "As a reminder, **your second line graph** should contain the following features:\n",
    "1. The X-axis should be the same as last time\n",
    "2. The Y-axis should be labelled \"Average Checkins\".\n",
    "3. There should be a title called \"Average Checkins of American (New) Restaurants for each Hour of Day\"\n",
    "4. There are 7 lines for each of the 7 days, clearly labeled and differentiated, based on **color** and **markers**.\n",
    "\n",
    "Hint: Again, make sure you are only considering restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7TyEQ1tpuMs"
   },
   "outputs": [],
   "source": [
    "# Second Plot:\n",
    "\n",
    "\n",
    "# Apply the filter for is_restaurant and categories\n",
    "plot2_df =\n",
    "\n",
    "\n",
    "# Now, alter this dataframe to have 3 columns, IN THIS ORDER:\n",
    "# 'day', 'Time of Day', 'Average Checkins'. Average Checkins is the mean checkins\n",
    "# for the particular hour of that day.\n",
    "\n",
    "\n",
    "# Now just plot the graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSIhhZUhRvul"
   },
   "source": [
    "## Part 4: Working with Text Data [8 points]\n",
    "\n",
    "Now, let's switch gears and try to text-based analysis. Textual data is complex, but can also be used to generate extremely interpretable results, making it both valuable and interesting.\n",
    "\n",
    "Throughout this section, we will attempt to answer the following question:\n",
    "\n",
    "**According to the `hotels_df` dataframe, what do the reviews for some of the most popular hotels in Europe look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVfjaKb9TXgn"
   },
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "hotels_df = pd.read_csv('hotel_reviews.csv')\n",
    "hotels_df.rename(columns={'Review_Total_Positive_Word_Counts':'reviews'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q548FKXpRxa1"
   },
   "source": [
    "### 4.1 Tokenizing the text [2 points]\n",
    "\n",
    "We are going to split the contents of the `reviews` column from `hotels_df` into a list of words. We will use the **nltk** library, which contains an extensive set of tools for text processing. Now, this homework would be interminably long if we went into all the details of nltk. Thus, we are only going to use the following components of the library:\n",
    "- `nltk.word_tokenize()`: a function used to tokenize text\n",
    "- `nltk.corpus.stopwords`: a list of commonly used words such as \"a\", \"an\",\"in\" that are often ignored in text analysis\n",
    "\n",
    "Note that for this question, we didn't have to clean the text data first as our original dataset was well-formatted. However, in practice, we would typically clean the text first using regular expressions (regex). Keep this in mind as you work on the project later on in the semester.\n",
    "\n",
    "**TODO:** Perform the following tasks:\n",
    "- Use **nltk.corpus.stopwords** to create a set containing the most common English stopwords.\n",
    "- Implement the function **tokenized_content(content)**, which takes in a string and does the following:\n",
    "1. Tokenize the text\n",
    "2. Keep tokens that only contain alphabetic characters (i.e. tokens with no punctuation)\n",
    "3. Convert each token to lowercase\n",
    "4. Remove stopwords (commonly used words such as \"a\", \"an\", \"in\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6Dj7lK6RwJo"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7i5GyzbTRze0"
   },
   "outputs": [],
   "source": [
    "# TODO: tokenize and flatten\n",
    "# (create a tokenized_content function that performs the steps listed above)\n",
    "\n",
    "def tokenized_content(content):\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5l8v8TvpR2f0"
   },
   "source": [
    "**TODO**: Now perform the following tasks:\n",
    "- We are interested in reviews for the **most reviewed** hotel in `hotels_df`. Using Counter() to find this hotel is recommended. Store the name of the hotel in the `most_reviewed_hotel` variable, and use it to create `hotel_reviews_df`, a subset of `hotels_df` only containing instances from the hotel in question.\n",
    "- The `most_reviewed_hotel` is calculated based on how many times the name of hotel showed up in the dataset.\n",
    "- From there, extract the `Positive_Review` column of `hotels_reviews_df` as a list called `reviews`.\n",
    "- Apply your `tokenize_content()` function to each item in the list `reviews`. Call the resultant list `top_tokens_list`. This will be a list of lists that store words for a single tokenized sentence.\n",
    "- Flatten the list of lists:`top_tokens_list`, and call the resultant list `top_tokens`. The autograder will be examining the contents of this list. (Flatten using list comprehension)\n",
    "\n",
    "For futher assistance, here is the documentation for Counter() objects:\n",
    "\n",
    "https://docs.python.org/2/library/collections.html#counter-objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HEfBgrtR45L"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# TODO: Find the most reviewed hotel, use that to make hotel_reviews_df,\n",
    "#       extract the reviews, use your function to make the token list, and flatten it\n",
    "\n",
    "most_reviewed_hotel =\n",
    "hotel_reviews_df =\n",
    "reviews =\n",
    "top_tokens_list =\n",
    "top_tokens ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CygSPqFpR7bs"
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "grader.grade(test_case_id = 'test_top_tokens', answer = top_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kj3yR5lR8ol"
   },
   "source": [
    "\n",
    "### 4.2 Most Frequent Words [2 points]\n",
    "**TODO**: Now, find the 10 most common words amongst the content of `top_tokens`. Return this as a list of `(word, count)` tuples called `top_most_common`, in descending order of `count`.\n",
    "\n",
    "**Hint**: We again recommend using `Counter` in this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0NA1buyR94q"
   },
   "outputs": [],
   "source": [
    "# TODO: Get top_most_common\n",
    "top_most_common ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2swTRq4R-3E"
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "grader.grade(test_case_id = 'test_top_most_common', answer = top_most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkr6CWbaR_yt"
   },
   "source": [
    "### 4.3 Word Clouds [4 points - manually graded]\n",
    "\n",
    "Before we move on from this dataset, let's visualize our results using a word cloud.\n",
    "\n",
    "**TODO**: Create a word cloud containing all the words in the list `top_tokens` (created in part 4.1). [The WordCloud documentation](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html) contains instructions on how to do this.\n",
    "\n",
    "*Please make sure your wordcloud has a **lavender** background.*\n",
    "\n",
    "We will be going through your notebooks and **manually grading** your word cloud. This is worth 4 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljvc8MUFSBWB"
   },
   "outputs": [],
   "source": [
    "# TODO: Generate your Word Cloud, making sure it meets the requirements above\n",
    "wordcloud ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6qJd9XvSClI"
   },
   "source": [
    "# HW Submission\n",
    "\n",
    "<br>\n",
    "<center><img src = \"https://i.imgflip.com/79knab.jpg\" width= \"500\" align =\"center\"/></center>\n",
    "<br>\n",
    "\n",
    "Congratulations on finishing this homework! The good news is that similar to HW1, you basically know your score when you submit to Gradescope.\n",
    "However, this time, we will be manually grading your lineplots and wordclouds, so the autograder score is not final! Remember that we will also be checking for plagiarism, so please make sure to cite your sources (if any) by commenting the urls / links you looked at.\n",
    "\n",
    "Before you submit on Gradescope (you must submit your notebook to receive credit):\n",
    "\n",
    "1.   Please rerun your notebook on Colab by clicking \"Restart and Run-All\", and make sure there is nothing wrong with your notebook.\n",
    "2.   **Double check that you have the correct PennID (all numbers) saved in the autograder**.\n",
    "3. Make sure you've run all the PennGrader cells and have received a score.\n",
    "4. Go to the \"File\" tab at the top left, and click \"Download .ipynb\" + \"Download .py\". Please name the `.ipynb` and `.py` files **\"homework2.ipynb\"** and **\"homework2.py\"** respectively. Then, upload both the `.py` and `.ipynb` files to Gradescope.\n",
    "\n",
    "###Be sure to name your files correctly!!!\n",
    "\n",
    "**Please let the course staff know ASAP if you have any issues submitting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-Q0Tg58SDxh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
